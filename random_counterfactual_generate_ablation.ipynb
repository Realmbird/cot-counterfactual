{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3daa832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reciever_heads = [(29, 24),\n",
    " (17, 0),\n",
    " (24, 7),\n",
    " (25, 22),\n",
    " (23, 8),\n",
    " (18, 12),\n",
    " (23, 23),\n",
    " (21, 4),\n",
    " (19, 17),\n",
    " (18, 14),\n",
    " (30, 17),\n",
    " (19, 27),\n",
    " (28, 22),\n",
    " (1, 17),\n",
    " (27, 1),\n",
    " (24, 1),\n",
    " (26, 10),\n",
    " (26, 24),\n",
    " (1, 16),\n",
    " (24, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0d2693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found problem directory: math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\n",
      "Found problem directory: ['problem_6481', 'problem_4682', 'problem_3360', 'problem_4605', 'problem_2236', 'problem_1591', 'problem_4164', 'problem_2189', 'problem_2238', 'problem_3935', 'problem_6596', 'problem_3550', 'problem_2870', 'problem_4019', 'problem_2050', 'problem_6998', 'problem_3916', 'problem_2137', 'problem_3448', 'problem_330']\n",
      "Successfully loaded data for 20 problems.\n",
      "Found problem directory: math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\n",
      "Found problem directory: ['problem_4019', 'problem_2870', 'problem_3550', 'problem_3935', 'problem_6596', 'problem_2238', 'problem_2189', 'problem_4164', 'problem_1591', 'problem_2236', 'problem_4605', 'problem_3360', 'problem_6481', 'problem_4682', 'problem_3448', 'problem_2137', 'problem_330', 'problem_6998', 'problem_3916', 'problem_2050']\n",
      "Successfully loaded data for 20 problems.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "def process_problem_data(base_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Iterates through all problem directories, extracts problem statements\n",
    "\n",
    "    and sentences from `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "        base_path (str): The path to the directory containing all the problems\n",
    "\n",
    "                         (e.g., 'math-rollouts/.../correct_base_solution').\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        list: A list of dictionaries, where each dictionary contains the problem\n",
    "\n",
    "              and all sentences for a given problem directory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_problem_data = []\n",
    "\n",
    "\n",
    "    # Check if the base path exists\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "    print(f\"Found problem directory: {base_path}\")\n",
    "\n",
    "\n",
    "    # List all entries in the base directory\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "    print(f\"Found problem directory: {problem_dirs}\")\n",
    "\n",
    "\n",
    "    if not problem_dirs:\n",
    "\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "\n",
    "    # Iterate through each problem directory (e.g., problem_330, problem_1591)\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "\n",
    "       \n",
    "\n",
    "        # Define the file paths for the problem and chunks\n",
    "\n",
    "        problem_file = os.path.join(problem_path, \"problem.json\")\n",
    "\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "\n",
    "       \n",
    "\n",
    "        problem_text = \"\"\n",
    "\n",
    "        allsentences = []\n",
    "\n",
    "       \n",
    "\n",
    "        # Load the problem statement\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(problem_file, 'r') as f:\n",
    "\n",
    "                problem_data = json.load(f)\n",
    "\n",
    "                problem_text = problem_data.get(\"problem\", \"\")\n",
    "                problem_answer = problem_data.get(\"gt_answer\", \"\")\n",
    "                \n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load problem.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Load all sentences from chunks_labeled.json\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(chunks_file, 'r') as f:\n",
    "\n",
    "                chunks_data = json.load(f)\n",
    "\n",
    "                allsentences = [chunk[\"chunk\"] for chunk in chunks_data]\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load chunks_labeled.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Create a dictionary to store the extracted data\n",
    "\n",
    "        problem_info = {\n",
    "\n",
    "            \"problem_id\": problem_name,\n",
    "\n",
    "            \"problem_statement\": problem_text,\n",
    "\n",
    "            \"sentences\": allsentences,\n",
    "            \"answer\": problem_answer\n",
    "\n",
    "        }\n",
    "\n",
    "        all_problem_data.append(problem_info)\n",
    "\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "correct_all_prompt = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "# Now, `all_data` is a list of dictionaries. You can iterate through it.\n",
    "\n",
    "print(f\"Successfully loaded data for {len(correct_all_prompt)} problems.\")\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "incorrect_all_prompt = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "print(f\"Successfully loaded data for {len(incorrect_all_prompt)} problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae63cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompt = correct_all_prompt[:2] + incorrect_all_prompt[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9972b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 problems with selected chunk fields.\n",
      "Loaded 20 problems with selected chunk fields.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def process_problem_labels_minimal(base_path):\n",
    "    \"\"\"\n",
    "    Iterates through all problem directories, extracts selected fields from each chunk\n",
    "    in `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The path to the directory containing all the problems.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each with problem_id and a list of selected chunk fields.\n",
    "    \"\"\"\n",
    "    all_problem_data = []\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "        return all_problem_data\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    if not problem_dirs:\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "        return all_problem_data\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "        if not os.path.isfile(chunks_file):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(chunks_file, \"r\") as f:\n",
    "                chunks_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {problem_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract only the fields you care about from each chunk\n",
    "        selected_chunks = []\n",
    "        for chunk in chunks_data:\n",
    "            selected = {\n",
    "                \"function_tags\": chunk.get(\"function_tags\"),\n",
    "                \"chunk\": chunk.get(\"chunk\"),\n",
    "                \"accuracy\": chunk.get(\"accuracy\"),\n",
    "                \"resampling_importance_accuracy\": chunk.get(\"resampling_importance_accuracy\"),\n",
    "                \"resampling_importance_kl\": chunk.get(\"resampling_importance_kl\"),\n",
    "                \"counterfactual_importance_accuracy\": chunk.get(\"counterfactual_importance_accuracy\"),\n",
    "                \"counterfactual_importance_kl\": chunk.get(\"counterfactual_importance_kl\"),\n",
    "                \n",
    "                \"summary\": chunk.get(\"summary\"),\n",
    "            }\n",
    "            selected_chunks.append(selected)\n",
    "\n",
    "        all_problem_data.append({\n",
    "            \"problem_id\": problem_name,\n",
    "            \"chunks\": selected_chunks\n",
    "        })\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "# Example usage:\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "correct_all_labels = process_problem_labels_minimal(base_problem_dir)\n",
    "print(f\"Loaded {len(correct_all_labels)} problems with selected chunk fields.\")\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "incorrect_all_labels = process_problem_labels_minimal(base_problem_dir)\n",
    "print(f\"Loaded {len(incorrect_all_labels)} problems with selected chunk fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6abd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = correct_all_labels[:2] + incorrect_all_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d445572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" # Or any other suitable model\n",
    "\n",
    "mname = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84fb8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens_dict = {'additional_special_tokens': ['</think>']}\n",
    "# tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# # Get the token ID for </think> for use in generation\n",
    "# think_end_token_id = tokenizer.convert_tokens_to_ids('</think>')\n",
    "# stop_token_list = [tokenizer.eos_token_id, think_end_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5221844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a30d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 32 layers and 32 attention heads per layer\n",
      "Total possible heads: 1024\n",
      "Selected 20 random receiver heads:\n",
      "   1: Layer  1, Head 19\n",
      "   2: Layer  1, Head 22\n",
      "   3: Layer  1, Head 29\n",
      "   4: Layer  2, Head  1\n",
      "   5: Layer  5, Head 18\n",
      "   6: Layer  5, Head 31\n",
      "   7: Layer  6, Head 17\n",
      "   8: Layer  7, Head  4\n",
      "   9: Layer  8, Head 29\n",
      "  10: Layer 12, Head 23\n",
      "  11: Layer 13, Head 31\n",
      "  12: Layer 14, Head  3\n",
      "  13: Layer 14, Head  9\n",
      "  14: Layer 14, Head 28\n",
      "  15: Layer 15, Head 21\n",
      "  16: Layer 17, Head 19\n",
      "  17: Layer 17, Head 25\n",
      "  18: Layer 26, Head 27\n",
      "  19: Layer 27, Head  0\n",
      "  20: Layer 28, Head 23\n",
      "\n",
      "Generated receiver_heads list:\n",
      "receiver_heads = [(1, 19), (1, 22), (1, 29), (2, 1), (5, 18), (5, 31), (6, 17), (7, 4), (8, 29), (12, 23), (13, 31), (14, 3), (14, 9), (14, 28), (15, 21), (17, 19), (17, 25), (26, 27), (27, 0), (28, 23)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def generate_random_receiver_heads(model, num_heads_to_select=20, seed=42):\n",
    "    \"\"\"\n",
    "    Generate a random list of receiver heads from the model's architecture.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        num_heads_to_select: Number of random heads to select\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of (layer_idx, head_idx) tuples\n",
    "    \"\"\"\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Get model architecture info\n",
    "    num_layers = len(model.model.layers)\n",
    "    num_attention_heads = model.config.num_attention_heads\n",
    "    \n",
    "    print(f\"Model has {num_layers} layers and {num_attention_heads} attention heads per layer\")\n",
    "    print(f\"Total possible heads: {num_layers * num_attention_heads}\")\n",
    "    \n",
    "    # Generate all possible (layer, head) combinations\n",
    "    all_possible_heads = []\n",
    "    for layer_idx in range(num_layers):\n",
    "        for head_idx in range(num_attention_heads):\n",
    "            all_possible_heads.append((layer_idx, head_idx))\n",
    "    \n",
    "    # Randomly sample without replacement\n",
    "    if num_heads_to_select > len(all_possible_heads):\n",
    "        print(f\"Warning: Requested {num_heads_to_select} heads but only {len(all_possible_heads)} available\")\n",
    "        num_heads_to_select = len(all_possible_heads)\n",
    "    \n",
    "    random_heads = random.sample(all_possible_heads, num_heads_to_select)\n",
    "    \n",
    "    # Sort by layer first, then by head for easier reading\n",
    "    random_heads.sort(key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "    print(f\"Selected {len(random_heads)} random receiver heads:\")\n",
    "    for i, (layer, head) in enumerate(random_heads):\n",
    "        print(f\"  {i+1:2d}: Layer {layer:2d}, Head {head:2d}\")\n",
    "    \n",
    "    return random_heads\n",
    "\n",
    "# Generate random receiver heads\n",
    "random_receiver_heads = generate_random_receiver_heads(\n",
    "    model=model, \n",
    "    num_heads_to_select=20, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Use the random heads in your experiments\n",
    "print(f\"\\nGenerated receiver_heads list:\")\n",
    "print(f\"receiver_heads = {random_receiver_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b70ffca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 19),\n",
       " (1, 22),\n",
       " (1, 29),\n",
       " (2, 1),\n",
       " (5, 18),\n",
       " (5, 31),\n",
       " (6, 17),\n",
       " (7, 4),\n",
       " (8, 29),\n",
       " (12, 23),\n",
       " (13, 31),\n",
       " (14, 3),\n",
       " (14, 9),\n",
       " (14, 28),\n",
       " (15, 21),\n",
       " (17, 19),\n",
       " (17, 25),\n",
       " (26, 27),\n",
       " (27, 0),\n",
       " (28, 23)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_receiver_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781c73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize both embedding methods\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb846328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want do a loop through all the chunks append it to the current text set it through multiple rollouts and measure counterfactual importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761ccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55308955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_answer(text: str, ground_truth_answer: str):\n",
    "    \"\"\"\n",
    "    Extract answers and check if they match the ground truth.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract boxed answers from\n",
    "        ground_truth_answer: The correct answer to compare against\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list of answers, True if any answer matches ground truth)\n",
    "    \"\"\"\n",
    "    return ground_truth_answer in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac3da485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(model, inputs):\n",
    "    \"\"\"Get probability distribution from model.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits[0, -1, :]  # Last token logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8caf44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, inputs, layer=-1):\n",
    "    \"\"\"Get hidden state embeddings from a specific layer.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # Get embeddings and convert to float32 on GPU\n",
    "        embeddings = outputs.hidden_states[layer][0, -1, :].float()  # Convert to float32\n",
    "    del outputs\n",
    "    return embeddings  # Keep on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc10cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence_model, inputs, layer=-1):\n",
    "    \"\"\"Get sentence embeddings using SentenceTransformer.\"\"\"\n",
    "    embedding = sentence_model.encode(inputs, convert_to_tensor=True)\n",
    "    return embedding.float()  # Ensure float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c331e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embeddings(llm_model, sentence_model, tokenizer, text):\n",
    "    \"\"\"Get both LLM and sentence transformer embeddings for comparison.\"\"\"\n",
    "    \n",
    "    # LLM embedding (your current method)\n",
    "    llm_inputs = tokenizer(text, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    llm_embedding = get_embeddings(llm_model, llm_inputs)\n",
    "    \n",
    "    # Sentence transformer embedding\n",
    "    sent_embedding = get_sentence_embeddings(sentence_model, text)\n",
    "\n",
    "    # MOVE TO CPU IMMEDIATELY to free GPU memory\n",
    "    llm_embedding_cpu = llm_embedding.cpu()\n",
    "    sent_embedding_cpu = sent_embedding.cpu()\n",
    "    \n",
    "    # Clean up GPU tensors\n",
    "    del llm_embedding, sent_embedding\n",
    "    return llm_embedding_cpu, sent_embedding_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d46c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embeddings_batch(llm_model, sentence_model, tokenizer, texts):\n",
    "    \"\"\"Get both LLM and sentence transformer embeddings for multiple texts in batches.\"\"\"\n",
    "    #texts list\n",
    "    \n",
    "    # Batch LLM embeddings\n",
    "    llm_inputs = tokenizer(texts, return_tensors=\"pt\", max_length=2048, truncation=True, padding=True)\n",
    "    device = next(llm_model.parameters()).device\n",
    "    llm_inputs = {k: v.to(device) for k, v in llm_inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model(**llm_inputs, output_hidden_states=True)\n",
    "        # Get last token embeddings for each sequence\n",
    "        llm_embeddings = outputs.hidden_states[-1][:, -1, :].float().cpu()\n",
    "    \n",
    "    # Batch sentence transformer embeddings\n",
    "    sent_embeddings = sentence_model.encode(texts, convert_to_tensor=True, batch_size=32).float().cpu()\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del outputs, llm_inputs\n",
    "    \n",
    "    return llm_embeddings, sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4cd4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0eb2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ecb2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_answers(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract answers enclosed in \\boxed{} from the text with improved handling\n",
    "    of nested braces and complex LaTeX expressions.\n",
    "\n",
    "    Args:\n",
    "        text: The text to extract boxed answers from\n",
    "\n",
    "    Returns:\n",
    "        List of extracted boxed answers\n",
    "    \"\"\"\n",
    "    # Find all occurrences of \\boxed{\n",
    "    boxed_starts = [m.start() for m in re.finditer(r\"\\\\boxed\\{\", text)]\n",
    "\n",
    "    if not boxed_starts:\n",
    "        return [\"\"]\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    for start_idx in boxed_starts:\n",
    "        # Start after \\boxed{\n",
    "        idx = start_idx + 7\n",
    "        brace_count = 1  # We've already opened one brace\n",
    "        answer = \"\"\n",
    "\n",
    "        # Parse until we find the matching closing brace\n",
    "        while idx < len(text) and brace_count > 0:\n",
    "            char = text[idx]\n",
    "\n",
    "            if char == \"{\":\n",
    "                brace_count += 1\n",
    "            elif char == \"}\":\n",
    "                brace_count -= 1\n",
    "\n",
    "                # Skip the closing brace of \\boxed{}\n",
    "                if brace_count == 0:\n",
    "                    break\n",
    "\n",
    "            if brace_count > 0:  # Only add if we're still inside the boxed content\n",
    "                answer += char\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        if answer:\n",
    "            answers.append(answer)\n",
    "\n",
    "    return answers if answers else [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0335a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(answer: str, use_sympy: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Get the final normalized and cleaned version of an answer.\n",
    "    This function combines all normalization steps used in check_answer.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to normalize\n",
    "        use_sympy: Whether to use sympy to normalize the answer\n",
    "\n",
    "    Returns:\n",
    "        The normalized answer string\n",
    "    \"\"\"\n",
    "    # First apply basic LaTeX normalization\n",
    "    normalized = normalize_latex(answer)\n",
    "\n",
    "    # Also prepare the answer for sympy if applicable\n",
    "    if use_sympy:\n",
    "        try:\n",
    "            sympy_ready = prepare_latex_for_sympy(answer)\n",
    "            if sympy_ready != normalized and len(sympy_ready) > 0:\n",
    "                return sympy_ready\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a67cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_equivalent(answer0, answer1):\n",
    "    \"\"\"\n",
    "    Check if two LaTeX expressions are mathematically equivalent using SymPy.\n",
    "\n",
    "    Args:\n",
    "        answer0: First LaTeX expression\n",
    "        answer1: Second LaTeX expression\n",
    "\n",
    "    Returns:\n",
    "        True if expressions are mathematically equivalent, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sympy.parsing.latex import parse_latex\n",
    "        import sympy\n",
    "\n",
    "        # Clean up the LaTeX expressions for parsing\n",
    "        answer0 = prepare_latex_for_sympy(answer0)\n",
    "        answer1 = prepare_latex_for_sympy(answer1)\n",
    "\n",
    "        # Parse the LaTeX expressions\n",
    "        expr1 = parse_latex(answer0)\n",
    "        expr2 = parse_latex(answer1)\n",
    "\n",
    "        # Check if they are mathematically identical\n",
    "        equals = expr1.equals(expr2)\n",
    "        # print(f\"First: {answer0}, Second: {answer1}: equals={equals}\")\n",
    "        return equals\n",
    "    except Exception as e:\n",
    "        # print(f\"Error comparing expressions: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def prepare_latex_for_sympy(latex_str):\n",
    "    \"\"\"\n",
    "    Prepare a LaTeX string for SymPy parsing by removing unsupported commands\n",
    "    and simplifying the expression.\n",
    "    \"\"\"\n",
    "    if not isinstance(latex_str, str):\n",
    "        return str(latex_str)\n",
    "\n",
    "    # Remove \\boxed{} command\n",
    "    latex_str = re.sub(r\"\\\\boxed\\{(.*?)\\}\", r\"\\1\", latex_str)\n",
    "\n",
    "    # Replace common LaTeX commands that SymPy doesn't support\n",
    "    replacements = {\n",
    "        r\"\\\\dfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\tfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\cdot\": r\"*\",\n",
    "        r\"\\\\times\": r\"*\",\n",
    "        r\"\\\\div\": r\"/\",\n",
    "        r\"\\\\left\": r\"\",\n",
    "        r\"\\\\right\": r\"\",\n",
    "        r\"\\\\textbf\": r\"\",\n",
    "        r\"\\\\text\": r\"\",\n",
    "        r\"\\\\mathrm\": r\"\",\n",
    "        r\"\\\\!\": r\"\",\n",
    "        r\",\": r\"\",\n",
    "    }\n",
    "\n",
    "    for old, new in replacements.items():\n",
    "        latex_str = re.sub(old, new, latex_str)\n",
    "\n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67b1b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_latex(latex_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize LaTeX string by applying various transformations.\n",
    "\n",
    "    Args:\n",
    "        latex_str: The LaTeX string to normalize\n",
    "\n",
    "    Returns:\n",
    "        Normalized LaTeX string\n",
    "    \"\"\"\n",
    "    normalized = latex_str.strip().lower()\n",
    "\n",
    "    # Replace different fraction notations\n",
    "    normalized = normalized.replace(\"dfrac\", \"frac\")\n",
    "    normalized = normalized.replace(\"tfrac\", \"frac\")\n",
    "\n",
    "    # Normalize spaces\n",
    "    normalized = re.sub(r\"\\s+\", \"\", normalized)\n",
    "\n",
    "    # Normalize percentages\n",
    "    normalized = normalized.replace(\"\\\\%\", \"\")\n",
    "\n",
    "    # Normalize funny commas\n",
    "    normalized = normalized.replace(\"{,}\", \"\")\n",
    "\n",
    "    # Normalize common mathematical notations\n",
    "    normalized = normalized.replace(\"\\\\times\", \"*\")\n",
    "    normalized = normalized.replace(\"\\\\cdot\", \"*\")\n",
    "\n",
    "    # Normalize decimal representation\n",
    "    normalized = re.sub(r\"(\\d+)[\\.,](\\d+)\", r\"\\1.\\2\", normalized)\n",
    "\n",
    "    # Remove unnecessary braces in simple expressions\n",
    "    normalized = re.sub(r\"{([^{}]+)}\", r\"\\1\", normalized)\n",
    "\n",
    "    # Normalize common constants\n",
    "    normalized = normalized.replace(\"\\\\pi\", \"pi\")\n",
    "\n",
    "    # Remove LaTeX text commands\n",
    "    normalized = re.sub(r\"\\\\text\\{([^{}]+)\\}\", r\"\\1\", normalized)\n",
    "    normalized = re.sub(r\"\\\\mathrm\\{([^{}]+)\\}\", r\"\\1\", normalized)\n",
    "\n",
    "    # Normalize date formats (e.g., \"October 30\" vs \"October\\\\ 30\")\n",
    "    normalized = re.sub(r\"([a-z]+)\\\\+\\s*(\\d+)\", r\"\\1\\2\", normalized)\n",
    "    normalized = normalized.replace(\"\\\\text\", \"\")\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04490bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_latex_for_sympy(latex_str):\n",
    "    \"\"\"\n",
    "    Prepare a LaTeX string for SymPy parsing by removing unsupported commands\n",
    "    and simplifying the expression.\n",
    "    \"\"\"\n",
    "    if not isinstance(latex_str, str):\n",
    "        return str(latex_str)\n",
    "\n",
    "    # Remove \\boxed{} command\n",
    "    latex_str = re.sub(r\"\\\\boxed\\{(.*?)\\}\", r\"\\1\", latex_str)\n",
    "\n",
    "    # Replace common LaTeX commands that SymPy doesn't support\n",
    "    replacements = {\n",
    "        r\"\\\\dfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\tfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\cdot\": r\"*\",\n",
    "        r\"\\\\times\": r\"*\",\n",
    "        r\"\\\\div\": r\"/\",\n",
    "        r\"\\\\left\": r\"\",\n",
    "        r\"\\\\right\": r\"\",\n",
    "        r\"\\\\textbf\": r\"\",\n",
    "        r\"\\\\text\": r\"\",\n",
    "        r\"\\\\mathrm\": r\"\",\n",
    "        r\"\\\\!\": r\"\",\n",
    "        r\",\": r\"\",\n",
    "    }\n",
    "\n",
    "    for old, new in replacements.items():\n",
    "        latex_str = re.sub(old, new, latex_str)\n",
    "\n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d10d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(answer: str, gt_answer: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the generated answer matches the ground truth answer\n",
    "    after normalizing LaTeX formatting.\n",
    "\n",
    "    Args:\n",
    "        answer: The generated answer to check\n",
    "        gt_answer: The ground truth answer to compare against\n",
    "\n",
    "    Returns:\n",
    "        True if the answers match after normalization, False otherwise\n",
    "    \"\"\"\n",
    "    # Normalize both answers\n",
    "    normalized_answer = normalize_latex(answer)\n",
    "    normalized_gt_answer = normalize_latex(gt_answer)\n",
    "\n",
    "    # First check if normalized strings match\n",
    "    if normalized_answer == normalized_gt_answer:\n",
    "        return True\n",
    "\n",
    "    # # If string comparison fails, try mathematical equivalence\n",
    "    # try:\n",
    "    #     return get_latex_equivalent(answer, gt_answer)\n",
    "    # except Exception as e:\n",
    "    #     # If SymPy parsing fails, fall back to string comparison result\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85c36094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_solution_into_chunks(solution_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a solution into chunks for rollout generation.\n",
    "\n",
    "    Args:\n",
    "        solution_text: The full solution text\n",
    "\n",
    "    Returns:\n",
    "        List of chunks\n",
    "    \"\"\"\n",
    "    # First, remove the prompt part if present\n",
    "    if \"<think>\" in solution_text:\n",
    "        solution_text = solution_text.split(\"<think>\")[1].strip()\n",
    "\n",
    "    # Remove the closing tag if present\n",
    "    if \"</think>\" in solution_text:\n",
    "        solution_text = solution_text.split(\"</think>\")[0].strip()\n",
    "\n",
    "    # Define patterns for chunk boundaries\n",
    "    sentence_ending_tokens = [\".\", \"?\", \"!\"]\n",
    "    paragraph_ending_patterns = [\"\\n\\n\", \"\\r\\n\\r\\n\"]\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # Process the text character by character\n",
    "    i = 0\n",
    "    while i < len(solution_text):\n",
    "        current_chunk += solution_text[i]\n",
    "\n",
    "        # Check for paragraph endings\n",
    "        is_paragraph_end = False\n",
    "        for pattern in paragraph_ending_patterns:\n",
    "            if (\n",
    "                i + len(pattern) <= len(solution_text)\n",
    "                and solution_text[i : i + len(pattern)] == pattern\n",
    "            ):\n",
    "                is_paragraph_end = True\n",
    "                break\n",
    "\n",
    "        # Check for sentence endings followed by space or newline\n",
    "        is_sentence_end = False\n",
    "        if i < len(solution_text) - 1 and solution_text[i] in sentence_ending_tokens:\n",
    "            next_char = solution_text[i + 1]\n",
    "            if next_char == \" \" or next_char == \"\\n\":\n",
    "                is_sentence_end = True\n",
    "\n",
    "        # If we found a boundary, add the chunk and reset\n",
    "        if is_paragraph_end or is_sentence_end:\n",
    "            if current_chunk.strip():\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # # Add the last chunk if not empty\n",
    "    # if current_chunk.strip():\n",
    "    #     chunks.append(current_chunk.strip())\n",
    "    #     chunk_idxs.append(len(solution_text) - 1)  # Add last index\n",
    "\n",
    "    # Merge small chunks (less than 10 characters)\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        if len(chunks[i]) < 10:\n",
    "            # If this is the last chunk, merge with previous chunk if possible\n",
    "            if i == len(chunks) - 1:\n",
    "                if i > 0:\n",
    "                    chunks[i - 1] = chunks[i - 1] + \" \" + chunks[i]\n",
    "                    chunks.pop(i)\n",
    "            # Otherwise merge with the next chunk\n",
    "            else:\n",
    "                chunks[i + 1] = chunks[i] + \" \" + chunks[i + 1]\n",
    "                chunks.pop(i)\n",
    "                # Don't increment i since we need to check the new merged chunk\n",
    "            # If we're at the beginning and there's only one chunk, just keep it\n",
    "            if i == 0 and len(chunks) == 1:\n",
    "                break\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # chunk_boundaries = [(chunk_idxs[i], chunk_idxs[i + 1]) for i in range(len(chunk_idxs) - 1)]\n",
    "    # chunk_boundaries.append((chunk_idxs[-1], len(solution_text)))\n",
    "\n",
    "    # if get_idxs:\n",
    "    #     return chunks, chunk_boundaries\n",
    "    # else:\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90770fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grouped_answer_kl_divergence(rollout_answer_correct, cos_sims, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between answer correctness distributions for similar vs dissimilar rollouts.\n",
    "    This follows the thought-anchors approach.\n",
    "    \n",
    "    Args:\n",
    "        rollout_answer_correct: List of boolean correctness for each rollout\n",
    "        cos_sims: Tensor of cosine similarities for each rollout\n",
    "        similarity_threshold: Threshold for determining similar vs dissimilar\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains KL divergences and group statistics\n",
    "    \"\"\"\n",
    "    # Ensure cos_sims is a tensor\n",
    "    if not torch.is_tensor(cos_sims):\n",
    "        cos_sims = torch.tensor(cos_sims, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    # Separate rollouts into similar and dissimilar groups\n",
    "    similar_mask = cos_sims > similarity_threshold\n",
    "    dissimilar_mask = ~similar_mask\n",
    "    \n",
    "    similar_correctness = [rollout_answer_correct[i] for i in range(len(rollout_answer_correct)) if similar_mask[i]]\n",
    "    dissimilar_correctness = [rollout_answer_correct[i] for i in range(len(rollout_answer_correct)) if dissimilar_mask[i]]\n",
    "    \n",
    "    if len(similar_correctness) == 0 or len(dissimilar_correctness) == 0:\n",
    "        return {\n",
    "            \"kl_divergence\": 0.0,\n",
    "            \"similar_group_size\": len(similar_correctness),\n",
    "            \"dissimilar_group_size\": len(dissimilar_correctness),\n",
    "            \"similar_accuracy\": 0.0,\n",
    "            \"dissimilar_accuracy\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Calculate accuracy rates for each group\n",
    "    similar_accuracy = sum(similar_correctness) / len(similar_correctness)\n",
    "    dissimilar_accuracy = sum(dissimilar_correctness) / len(dissimilar_correctness)\n",
    "    \n",
    "    # Create probability distributions\n",
    "    # Similar distribution: [P(wrong), P(correct)]\n",
    "    similar_dist = torch.tensor([1 - similar_accuracy, similar_accuracy], dtype=torch.float32)\n",
    "    \n",
    "    # Dissimilar distribution: [P(wrong), P(correct)]\n",
    "    dissimilar_dist = torch.tensor([1 - dissimilar_accuracy, dissimilar_accuracy], dtype=torch.float32)\n",
    "    \n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-8\n",
    "    similar_dist = similar_dist + eps\n",
    "    dissimilar_dist = dissimilar_dist + eps\n",
    "    \n",
    "    # Normalize to ensure they sum to 1\n",
    "    similar_dist = similar_dist / similar_dist.sum()\n",
    "    dissimilar_dist = dissimilar_dist / dissimilar_dist.sum()\n",
    "    \n",
    "    # Calculate KL divergence: KL(P_dissimilar || P_similar)\n",
    "    # This measures how much the dissimilar group diverges from similar group\n",
    "    kl_div = torch.sum(dissimilar_dist * torch.log(dissimilar_dist / similar_dist))\n",
    "    \n",
    "    return {\n",
    "        \"kl_divergence\": float(kl_div),\n",
    "        \"similar_group_size\": len(similar_correctness),\n",
    "        \"dissimilar_group_size\": len(dissimilar_correctness),\n",
    "        \"similar_accuracy\": similar_accuracy,\n",
    "        \"dissimilar_accuracy\": dissimilar_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f43a96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_max_tokens_for_sentence(sentence_idx):\n",
    "#     \"\"\"Dynamic token allocation based on sentence position.\"\"\"\n",
    "#     if sentence_idx < 20:\n",
    "#         return 6000  # Full CoT for very early sentences\n",
    "#     elif sentence_idx < 40:\n",
    "#         return 5000  # Slightly reduced\n",
    "#     elif sentence_idx < :\n",
    "#         return 4500  # Moderate reduction\n",
    "#     else:\n",
    "#         return 4000  # Conservative for very late sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a68f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diverse_rollouts(model, tokenizer, ground_truth_answer, context, num_rollouts=10, batch_size=5, temperature=0.6, top_p=0.95, sentence_idx=0):\n",
    "    \"\"\"Generate diverse text completions in batches for better GPU utilization.\"\"\"\n",
    "\n",
    "    # START: Clean memory before beginning (NEW)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", max_length=1500, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Dynamic token allocation based on sentence position\n",
    "    max_new_tokens = 6000\n",
    "\n",
    "    # Add </think> as a stop sequence\n",
    "    stop_token_ids = tokenizer.encode(\"</think>\", add_special_tokens=False)\n",
    "    rollout_texts = []\n",
    "    rollout_answer_correct = [] # this uses contains_answer\n",
    "    rollout_answer_correct_check = []  # New list using check_answer\n",
    "    \n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in range(0, num_rollouts, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_rollouts)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        \n",
    "        # Expand inputs for batch processing\n",
    "        batch_inputs = {\n",
    "            'input_ids': inputs['input_ids'].repeat(current_batch_size, 1),\n",
    "            'attention_mask': inputs['attention_mask'].repeat(current_batch_size, 1)\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                batch_inputs['input_ids'],\n",
    "                attention_mask=batch_inputs['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                output_hidden_states=True,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "\n",
    "        # Process each sequence in the batch\n",
    "        batch_generated_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        batch_generated_texts = tokenizer.batch_decode(batch_generated_ids, skip_special_tokens=True)\n",
    "        # 2. BATCH STRIP ALL TEXTS\n",
    "        batch_texts_stripped = [text.strip() for text in batch_generated_texts]\n",
    "        rollout_texts.extend(batch_texts_stripped)\n",
    "        \n",
    "        # 3. BATCH CHECK ANSWER CORRECTNESS (contains_answer)\n",
    "        batch_contains_answer = [contains_answer(text, ground_truth_answer) for text in batch_texts_stripped]\n",
    "        rollout_answer_correct.extend(batch_contains_answer)\n",
    "        \n",
    "        # 4. BATCH EXTRACT AND CHECK BOXED ANSWERS\n",
    "        batch_boxed_answers = [extract_boxed_answers(text) for text in batch_texts_stripped]\n",
    "        batch_correct_check = [\n",
    "            any(check_answer(answer, ground_truth_answer) for answer in boxed_answers)\n",
    "            for boxed_answers in batch_boxed_answers\n",
    "        ]\n",
    "        rollout_answer_correct_check.extend(batch_correct_check)\n",
    "                \n",
    "        # ADD THIS: Clean up after each batch\n",
    "        del outputs, batch_inputs, batch_generated_ids, batch_generated_texts\n",
    "        del batch_texts_stripped, batch_contains_answer, batch_boxed_answers, batch_correct_check\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    # Final cleanup\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Stack embeddings into a single tensor on GPU\n",
    "    return rollout_texts, rollout_answer_correct, rollout_answer_correct_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90c79f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_embedding():\n",
    "#     rollout_llm_embeddings = []\n",
    "#     rollout_sentence_embeddings = []\n",
    "#     rollout_sentences = []\n",
    "#     for rollout_text in rollout_texts:\n",
    "#         rollout_resampled = split_solution_into_chunks(rollout_text)[0]\n",
    "#         llm_embedding, sent_embedding = get_both_embeddings(model, sentence_model, tokenizer, rollout_resampled)\n",
    "#         rollout_llm_embeddings.append(llm_embedding)\n",
    "#         rollout_sentence_embeddings.append(sent_embedding)\n",
    "#         rollout_sentences.append(rollout_resampled)\n",
    "#     # Stack embeddings\n",
    "#     rollout_llm_embeddings = torch.stack(rollout_llm_embeddings)\n",
    "#     rollout_sentence_embeddings = torch.stack(rollout_sentence_embeddings)\n",
    "    \n",
    "#     # Calculate cosine similarities using PyTorch batch operations\n",
    "#     cos_sims_llm = torch.cosine_similarity(\n",
    "#         original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "#     )\n",
    "#     cos_sims_sent = torch.cosine_similarity(\n",
    "#         original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "#     )\n",
    "\n",
    "#     print(f\"cos_sims_llm{sentence_idx}: {cos_sims_llm}\")\n",
    "#     print(f\"cos_sims_sent{sentence_idx}: {cos_sims_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63a609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embeddings_batch(llm_model, sentence_model, tokenizer, texts):\n",
    "    \"\"\"Get both LLM and sentence transformer embeddings for multiple texts in batches.\"\"\"\n",
    "    \n",
    "    # Batch LLM embeddings\n",
    "    llm_inputs = tokenizer(texts, return_tensors=\"pt\", max_length=2048, truncation=True, padding=True)\n",
    "    device = next(llm_model.parameters()).device\n",
    "    llm_inputs = {k: v.to(device) for k, v in llm_inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model(**llm_inputs, output_hidden_states=True)\n",
    "        # Get last token embeddings for each sequence\n",
    "        llm_embeddings = outputs.hidden_states[-1][:, -1, :].float().cpu()\n",
    "    \n",
    "    # Batch sentence transformer embeddings\n",
    "    sent_embeddings = sentence_model.encode(texts, convert_to_tensor=True, batch_size=32).float().cpu()\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del outputs, llm_inputs\n",
    "    \n",
    "    return llm_embeddings, sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64cc9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_sentence(model, tokenizer, problem_text, allsentences, ground_truth_answer,  num_rollouts=20, sentence_idx=0, batch=5):\n",
    "    #sentence_idx sentence position\n",
    "    \"\"\"Save raw rollout data and similarities for later importance calculation.\"\"\"\n",
    "    \n",
    "    # START: Clean memory before beginning\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Context WITHOUT the current sentence\n",
    "    print(f\"Context removed: {allsentences[sentence_idx]}\")\n",
    "    prefix_text = allsentences[:sentence_idx]\n",
    "    context_without = problem_text + \" \" + \" \".join(prefix_text) + \"\"\n",
    "\n",
    "    # Generate diverse rollouts from context without chunk\n",
    "    rollout_texts, rollout_answer_correct, rollout_answer_correct_check = generate_diverse_rollouts(\n",
    "        model,                    \n",
    "        tokenizer,               \n",
    "        ground_truth_answer,     \n",
    "        context_without,         \n",
    "        num_rollouts=num_rollouts,    \n",
    "        batch_size=batch,             \n",
    "        temperature=0.6,              \n",
    "        top_p=0.95,\n",
    "        sentence_idx=sentence_idx                     \n",
    "    )\n",
    "    \n",
    "    # Get original sentence embeddings\n",
    "    original_text = allsentences[sentence_idx]\n",
    "    original_llm_emb, original_sent_emb = get_both_embeddings(model, sentence_model, tokenizer, original_text)\n",
    "\n",
    "    # BATCH PROCESS ROLLOUTS\n",
    "    rollout_sentences = [split_solution_into_chunks(rollout_text)[0] for rollout_text in rollout_texts]\n",
    "    \n",
    "    # Get all embeddings at once\n",
    "    rollout_llm_embeddings, rollout_sentence_embeddings = get_both_embeddings_batch(\n",
    "        model, sentence_model, tokenizer, rollout_sentences\n",
    "    )\n",
    "    \n",
    "    # Calculate cosine similarities using PyTorch batch operations\n",
    "    cos_sims_llm = torch.cosine_similarity(\n",
    "        original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "    )\n",
    "    cos_sims_sent = torch.cosine_similarity(\n",
    "        original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "    )\n",
    "\n",
    "    print(f\"cos_sims_llm{sentence_idx}: {cos_sims_llm}\")\n",
    "    print(f\"cos_sims_sent{sentence_idx}: {cos_sims_sent}\")\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    unique_responses = len(set(rollout_texts))\n",
    "    \n",
    "    # Store raw data for later importance calculation\n",
    "    result = {\n",
    "        \"problem_id\": None,  # Will be set by caller\n",
    "        \"sentence_idx\": sentence_idx,\n",
    "        \"sentence_text\": original_text,\n",
    "        \"function_tags\": [],  # Will be set by caller\n",
    "        \n",
    "        # Context information\n",
    "        \"context_without_sentence\": context_without,\n",
    "        \"ground_truth_answer\": ground_truth_answer,\n",
    "        \n",
    "        # Rollout data\n",
    "        \"num_rollouts\": num_rollouts,\n",
    "        \"rollout_sentences\": rollout_sentences,\n",
    "        \"rollout_answer_correct\": rollout_answer_correct,\n",
    "        \"rollout_answer_correct_check\": rollout_answer_correct_check,\n",
    "        \"unique_responses\": unique_responses,\n",
    "\n",
    "        \n",
    "        # extracted boxed_answers\n",
    "        \"rollout_boxed_answers\": [extract_boxed_answers(text) for text in rollout_texts],\n",
    "        # Raw similarity scores (convert to lists for JSON serialization)\n",
    "        \"cos_sims_llm\": cos_sims_llm.cpu().tolist(),\n",
    "        \"cos_sims_sentence\": cos_sims_sent.cpu().tolist(),\n",
    "        \n",
    "        # Summary statistics for quick reference\n",
    "        # \"llm_similarity_stats\": {\n",
    "        #     \"mean\": float(torch.mean(cos_sims_llm)),\n",
    "        #     \"std\": float(torch.std(cos_sims_llm)),\n",
    "        #     \"min\": float(torch.min(cos_sims_llm)),\n",
    "        #     \"max\": float(torch.max(cos_sims_llm))\n",
    "        # },\n",
    "        # \"sentence_similarity_stats\": {\n",
    "        #     \"mean\": float(torch.mean(cos_sims_sent)),\n",
    "        #     \"std\": float(torch.std(cos_sims_sent)),\n",
    "        #     \"min\": float(torch.min(cos_sims_sent)),\n",
    "        #     \"max\": float(torch.max(cos_sims_sent))\n",
    "        # },\n",
    "        \n",
    "        # Embedding comparison\n",
    "        \n",
    "        # Store original embeddings for potential later use (optional)\n",
    "        # \"original_llm_embedding\": original_llm_emb.cpu().tolist(),\n",
    "        # \"original_sent_embedding\": original_sent_emb.cpu().tolist(),\n",
    "        \n",
    "        # Metadata for importance calculation\n",
    "        \"generation_params\": {\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95,\n",
    "            \"batch_size\": batch\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    del rollout_texts, rollout_llm_embeddings, rollout_sentence_embeddings, rollout_answer_correct, rollout_answer_correct_check\n",
    "    del cos_sims_llm, cos_sims_sent, rollout_sentences\n",
    "    del original_llm_emb, original_sent_emb\n",
    "    if 'original_text' in locals():\n",
    "        del original_text\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a627ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_tags = [\n",
    "#         'uncertainty_management', \n",
    "#         'plan_generation'\n",
    "#     ]\n",
    "# def extract_target_sentence_indices(all_problem_labels, target_tags):\n",
    "#     \"\"\"\n",
    "#     Extract sentence indices that have the target function tags.\n",
    "    \n",
    "#     Returns a dictionary mapping problem_id to list of sentence indices to process.\n",
    "#     \"\"\"\n",
    "#     target_indices = {}\n",
    "    \n",
    "#     for problem in all_problem_labels:\n",
    "#         problem_id = problem['problem_id']\n",
    "#         indices_to_process = []\n",
    "        \n",
    "#         for i, chunk_data in enumerate(problem['chunks']):\n",
    "#             function_tags = chunk_data.get('function_tags', [])\n",
    "            \n",
    "#             # Check if any target tags are in this chunk's function_tags\n",
    "#             if any(tag in function_tags for tag in target_tags):\n",
    "#                 indices_to_process.append(i)\n",
    "        \n",
    "#         if indices_to_process:  # Only add if we found relevant chunks\n",
    "#             target_indices[problem_id] = indices_to_process\n",
    "    \n",
    "#     return target_indices\n",
    "\n",
    "# # Extract target sentence indices\n",
    "# target_sentence_indices = extract_target_sentence_indices(all_problem_labels, target_tags)\n",
    "# target_sentence_indices = extract_target_sentence_indices(all_problem_labels, target_tags)\n",
    "\n",
    "# print(f\"Found target sentences in {len(target_sentence_indices)} problems\")\n",
    "\n",
    "# for prompt, label in zip(all_prompt[:1], all_problem_labels[:1]):\n",
    "#     problem_id = prompt[\"problem_id\"]\n",
    "#     problem_text_prompt = prompt[\"problem_statement\"]\n",
    "#     allsentences = prompt[\"sentences\"]\n",
    "#     ground_truth_answer = prompt[\"answer\"]\n",
    "#     problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "#     # Get target sentence indices for this problem\n",
    "#     if problem_id not in target_sentence_indices:\n",
    "#         print(f\"\\nSkipping problem {problem_id} - no target function tags found\")\n",
    "#         continue\n",
    "    \n",
    "#     target_indices = target_sentence_indices[problem_id]\n",
    "#     print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} total sentences\")\n",
    "#     print(f\"Target sentence indices to process: {target_indices}\")\n",
    "    \n",
    "#     # Process only target sentences\n",
    "#     sentence_results = []\n",
    "#     for sentence_idx in target_indices:\n",
    "#         if sentence_idx >= len(allsentences):\n",
    "#             print(f\"Warning: sentence_idx {sentence_idx} >= len(allsentences) {len(allsentences)}\")\n",
    "#             continue\n",
    "            \n",
    "#         print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} (target) ---\")\n",
    "#         print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "#         print(f\"Function tags: {label['chunks'][sentence_idx].get('function_tags', [])}\")\n",
    "        \n",
    "#         sentence_result = process_one_sentence(\n",
    "#             model, tokenizer, problem_text, allsentences, ground_truth_answer, \n",
    "#             num_rollouts=num_rollouts, sentence_idx=sentence_idx, batch=batch\n",
    "#         )\n",
    "        \n",
    "#         # Add problem metadata and function tags\n",
    "#         sentence_result[\"problem_id\"] = problem_id\n",
    "#         sentence_result[\"sentence_idx\"] = sentence_idx\n",
    "#         sentence_result[\"sentence_text\"] = allsentences[sentence_idx]\n",
    "#         sentence_result[\"function_tags\"] = label['chunks'][sentence_idx].get('function_tags', [])\n",
    "#         sentence_results.append(sentence_result)\n",
    "        \n",
    "#         # Cleanup after each sentence\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "#     results.extend(sentence_results)\n",
    "#     print(f\"\\nCompleted problem {problem_id} - processed {len(sentence_results)} target sentences\")\n",
    "\n",
    "# print(f\"\\nProcessed {len(results)} target sentences across all problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c12118ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_problem(\n",
    "    problem_data: Dict, \n",
    "    problem_labels: Dict,\n",
    "    model, \n",
    "    tokenizer, \n",
    "    sentence_model,\n",
    "    output_dir: str = \"rollout_results\",\n",
    "    num_rollouts: int = 20,\n",
    "    batch_size: int = 5,\n",
    "    force: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single problem: loop through every sentence and generate rollouts.\n",
    "    Can resume from where it left off if results already exist.\n",
    "    \"\"\"\n",
    "    problem_id = problem_data[\"problem_id\"]\n",
    "    problem_text_prompt = problem_data[\"problem_statement\"]\n",
    "    allsentences = problem_data[\"sentences\"]\n",
    "    ground_truth_answer = problem_data[\"answer\"]\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_path = Path(output_dir)\n",
    "    problem_dir = output_path / problem_id\n",
    "    problem_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Define the results file path\n",
    "    results_file = problem_dir / \"sentence_rollouts.json\"\n",
    "    \n",
    "    # Check if we can resume from existing results\n",
    "    sentence_results = []\n",
    "    start_sentence_idx = 0\n",
    "    \n",
    "    if results_file.exists() and not force:\n",
    "        try:\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                existing_results = json.load(f)\n",
    "            \n",
    "            # Filter out failed results to find the last successful sentence\n",
    "            successful_results = [r for r in existing_results if \"error\" not in r]\n",
    "            \n",
    "            if successful_results:\n",
    "                sentence_results = existing_results\n",
    "                start_sentence_idx = len(successful_results)\n",
    "                print(f\"Resuming from sentence {start_sentence_idx + 1}/{len(allsentences)}\")\n",
    "                print(f\"Found {len(successful_results)} existing successful results\")\n",
    "            else:\n",
    "                print(f\"Found existing file but no successful results. Starting from beginning.\")\n",
    "                \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error reading existing results file: {e}. Starting from beginning.\")\n",
    "    \n",
    "    # Prepare problem text for generation\n",
    "    problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "    print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} sentences\")\n",
    "    \n",
    "    # Process sentences starting from where we left off\n",
    "    for sentence_idx in range(start_sentence_idx, len(allsentences)):\n",
    "        print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} ---\")\n",
    "        print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "        \n",
    "        # Get function tags if available\n",
    "        function_tags = []\n",
    "        if sentence_idx < len(problem_labels.get('chunks', [])):\n",
    "            function_tags = problem_labels['chunks'][sentence_idx].get('function_tags', [])\n",
    "            print(f\"Function tags: {function_tags}\")\n",
    "        \n",
    "        try:\n",
    "            # Process this sentence\n",
    "            sentence_result = process_one_sentence(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                problem_text=problem_text,\n",
    "                allsentences=allsentences,\n",
    "                ground_truth_answer=ground_truth_answer,\n",
    "                num_rollouts=num_rollouts,\n",
    "                sentence_idx=sentence_idx,\n",
    "                batch=batch_size\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            sentence_result[\"problem_id\"] = problem_id\n",
    "            sentence_result[\"sentence_idx\"] = sentence_idx\n",
    "            sentence_result[\"sentence_text\"] = allsentences[sentence_idx]\n",
    "            sentence_result[\"function_tags\"] = function_tags\n",
    "            \n",
    "            sentence_results.append(sentence_result)\n",
    "            \n",
    "            print(f\"Sentence {sentence_idx + 1}: Completed successfully\")\n",
    "            print(f\"  - Unique responses: {sentence_result['unique_responses']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence {sentence_idx}: {e}\")\n",
    "            # Still save partial results\n",
    "            error_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": allsentences[sentence_idx],\n",
    "                \"function_tags\": function_tags,\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "            sentence_results.append(error_result)\n",
    "        \n",
    "        # Cleanup after each sentence\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save intermediate results every 5 sentences or immediately if we're resuming\n",
    "        if (sentence_idx + 1) % 5 == 0 or sentence_idx == start_sentence_idx:\n",
    "            with open(results_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(sentence_results, f, indent=2, default=str)\n",
    "            print(f\"Saved intermediate results after sentence {sentence_idx + 1}\")\n",
    "    \n",
    "    # Save final results\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sentence_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary = {\n",
    "        \"problem_id\": problem_id,\n",
    "        \"total_sentences\": len(allsentences),\n",
    "        \"processed_sentences\": len(sentence_results),\n",
    "        \"successful_sentences\": len([r for r in sentence_results if \"error\" not in r]),\n",
    "        \"failed_sentences\": len([r for r in sentence_results if \"error\" in r]),\n",
    "        \"generation_params\": {\n",
    "            \"num_rollouts\": num_rollouts,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95\n",
    "        },\n",
    "        \"resumed_from_sentence\": start_sentence_idx\n",
    "    }\n",
    "    \n",
    "    summary_file = problem_dir / \"processing_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for temp_file in problem_dir.glob(\"sentence_rollouts_temp_*.json\"):\n",
    "        temp_file.unlink()\n",
    "    \n",
    "    print(f\"\\nCompleted problem {problem_id}\")\n",
    "    print(f\"  - Total sentences: {len(allsentences)}\")\n",
    "    print(f\"  - Successfully processed: {summary['successful_sentences']}\")\n",
    "    print(f\"  - Failed: {summary['failed_sentences']}\")\n",
    "    print(f\"  - Results saved to: {results_file}\")\n",
    "    \n",
    "    return sentence_results\n",
    "\n",
    "def process_multiple_problems(\n",
    "    all_prompt: List[Dict],\n",
    "    all_problem_labels: List[Dict],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sentence_model,\n",
    "    output_dir: str = \"rollout_results\",\n",
    "    num_rollouts: int = 20,\n",
    "    batch_size: int = 5,\n",
    "    force: bool = False,\n",
    "    max_problems: int = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process multiple problems sequentially.\n",
    "    Can resume from partially completed problems.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Limit number of problems if specified\n",
    "    problems_to_process = all_prompt[:max_problems] if max_problems else all_prompt\n",
    "    labels_to_process = all_problem_labels[:max_problems] if max_problems else all_problem_labels\n",
    "    \n",
    "    print(f\"Processing {len(problems_to_process)} problems\")\n",
    "    \n",
    "    all_sentence_results = []\n",
    "    \n",
    "    for i, (problem_data, problem_labels) in enumerate(zip(problems_to_process, labels_to_process)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing problem {i+1}/{len(problems_to_process)}: {problem_data['problem_id']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            sentence_results = process_single_problem(\n",
    "                problem_data=problem_data,\n",
    "                problem_labels=problem_labels,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                sentence_model=sentence_model,\n",
    "                output_dir=output_dir,\n",
    "                num_rollouts=num_rollouts,\n",
    "                batch_size=batch_size,\n",
    "                force=force\n",
    "            )\n",
    "            all_sentence_results.extend(sentence_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process problem {problem_data['problem_id']}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Final cleanup between problems\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nCompleted processing all problems. Results saved in: {output_dir}\")\n",
    "    print(f\"Total sentence results: {len(all_sentence_results)}\")\n",
    "    \n",
    "    return all_sentence_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46d65c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test just the first sentence with truncation debugging\n",
    "# def debug_first_sentence():\n",
    "#     # Get data for first sentence\n",
    "#     problem_data = all_prompt[0]\n",
    "#     problem_labels = all_labels[0]\n",
    "    \n",
    "#     problem_id = problem_data[\"problem_id\"]\n",
    "#     problem_text_prompt = problem_data[\"problem_statement\"]\n",
    "#     allsentences = problem_data[\"sentences\"]\n",
    "#     ground_truth_answer = problem_data[\"answer\"]\n",
    "    \n",
    "#     problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "#     print(f\"Testing first sentence of problem {problem_id}\")\n",
    "#     print(f\"First sentence: {allsentences[0]}\")\n",
    "#     print(f\"Ground truth answer: {ground_truth_answer}\")\n",
    "#     print(\"=\"*50)\n",
    "    \n",
    "#     # Process just the first sentence (sentence_idx=0)\n",
    "#     sentence_result = process_one_sentence(\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         problem_text=problem_text,\n",
    "#         allsentences=allsentences,\n",
    "#         ground_truth_answer=ground_truth_answer,\n",
    "#         num_rollouts=10,  # Small number for testing\n",
    "#         sentence_idx=0,  # First sentence\n",
    "#         batch=5\n",
    "#     )\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"TRUNCATION ANALYSIS:\")\n",
    "#     print(\"=\"*50)\n",
    "    \n",
    "#     # Analyze each rollout for truncation\n",
    "#     for i, rollout_text in enumerate(sentence_result[\"rollout_texts\"]):\n",
    "#         tokens = tokenizer(rollout_text, return_tensors=\"pt\")['input_ids'].shape[1]\n",
    "        \n",
    "#         print(f\"\\nRollout {i+1}:\")\n",
    "#         print(f\"  Tokens: {tokens}\")\n",
    "#         print(f\"  Words: {len(rollout_text.split())}\")\n",
    "        \n",
    "#         # Check ending\n",
    "#         ending = rollout_text[-100:].replace('\\n', ' ').strip()\n",
    "#         print(f\"  Last 100 chars: ...{ending}\")\n",
    "        \n",
    "#         # Truncation indicators\n",
    "#         truncation_signs = [\n",
    "#             rollout_text.strip().endswith(','),\n",
    "#             rollout_text.strip().endswith('='),\n",
    "#             rollout_text.strip().endswith('+'),\n",
    "#             rollout_text.strip().endswith('but'),\n",
    "#             rollout_text.strip().endswith('So'),\n",
    "#             rollout_text.strip().endswith('The'),\n",
    "#             rollout_text.strip().endswith('('),\n",
    "#         ]\n",
    "        \n",
    "#         has_proper_ending = (\n",
    "#             '\\\\boxed{' in rollout_text or \n",
    "#             rollout_text.strip().endswith('</think>') or\n",
    "#             rollout_text.strip().endswith('.')\n",
    "#         )\n",
    "        \n",
    "#         is_truncated = (tokens >= 3995 or any(truncation_signs) or not has_proper_ending)\n",
    "        \n",
    "#         print(f\"  Has proper ending: {has_proper_ending}\")\n",
    "#         print(f\"  Truncation signs: {any(truncation_signs)}\")\n",
    "#         print(f\"  Is truncated: {is_truncated}\")\n",
    "#         print(f\"  Contains answer: {ground_truth_answer in rollout_text}\")\n",
    "    \n",
    "#     return sentence_result\n",
    "\n",
    "# # # Run the debug test\n",
    "# # debug_result = debug_first_sentence()\n",
    "# for i, rollout_text in enumerate(debug_result[\"rollout_texts\"]):\n",
    "#     boxed_answers = extract_boxed_answers(rollout_text)\n",
    "#     print(f\"Rollout {i+1} boxed answers: {boxed_answers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47bb24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rollouts = 6\n",
    "# batch_size = 6\n",
    "# outputdir = \"rollout_results_no_ablation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8afd48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# process_multiple_problems(\n",
    "#     all_prompt=all_prompt[:1],\n",
    "#     all_problem_labels=all_labels[:1],\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     sentence_model=sentence_model,\n",
    "#      output_dir= outputdir,\n",
    "#     num_rollouts=num_rollouts,\n",
    "#     batch_size=batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeffa338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_problem_with_multi_head_ablation(\n",
    "    problem_data: Dict, \n",
    "    problem_labels: Dict,\n",
    "    model, \n",
    "    tokenizer, \n",
    "    sentence_model,\n",
    "    receiver_heads: List[tuple],  # List of (layer_idx, head_idx) tuples\n",
    "    output_dir: str = \"rollout_results_ablation\",\n",
    "    num_rollouts: int = 6,\n",
    "    batch_size: int = 6,\n",
    "    force: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single problem with receiver head ablation: loop through every sentence \n",
    "    and ablate ALL receiver heads simultaneously, storing rollouts and similarities.\n",
    "    \"\"\"\n",
    "    problem_id = problem_data[\"problem_id\"]\n",
    "    problem_text_prompt = problem_data[\"problem_statement\"]\n",
    "    allsentences = problem_data[\"sentences\"]\n",
    "    ground_truth_answer = problem_data[\"answer\"]\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_path = Path(output_dir)\n",
    "    problem_dir = output_path / problem_id\n",
    "    problem_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Prepare problem text for generation\n",
    "    problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "    print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} sentences\")\n",
    "    print(f\"Will ablate {len(receiver_heads)} receiver heads SIMULTANEOUSLY per sentence\")\n",
    "    \n",
    "    all_ablation_results = []\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence_idx in range(len(allsentences)):\n",
    "        print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} ---\")\n",
    "        print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "        \n",
    "        # Get context without current sentence (same as baseline)\n",
    "        prefix_text = allsentences[:sentence_idx]\n",
    "        context_without = problem_text + \" \" + \" \".join(prefix_text) + \"\"\n",
    "        \n",
    "        # Get function tags if available\n",
    "        function_tags = []\n",
    "        if sentence_idx < len(problem_labels.get('chunks', [])):\n",
    "            function_tags = problem_labels['chunks'][sentence_idx].get('function_tags', [])\n",
    "        \n",
    "        # Get original sentence embeddings\n",
    "        original_text = allsentences[sentence_idx]\n",
    "        original_llm_emb, original_sent_emb = get_both_embeddings(model, sentence_model, tokenizer, original_text)\n",
    "        \n",
    "        # CREATE ABLATION HOOKS FOR ALL HEADS\n",
    "        hooks = []\n",
    "        print(f\"  Adding hooks for {len(receiver_heads)} heads...\")\n",
    "        \n",
    "        for layer_idx, head_num in receiver_heads:\n",
    "            print(f\"    Adding hook for head ({layer_idx}, {head_num})\")\n",
    "            \n",
    "            # Define ablation hook for this specific head\n",
    "            def create_ablation_hook(target_head_num):\n",
    "                def ablation_hook(module, input, output):\n",
    "                    attention_output = output[0]\n",
    "                    batch_size_tensor, seq_len, hidden_dim = attention_output.shape\n",
    "                    num_heads = module.num_attention_heads\n",
    "                    head_dim = hidden_dim // num_heads\n",
    "                    \n",
    "                    # Zero out the specific head\n",
    "                    reshaped = attention_output.view(batch_size_tensor, seq_len, num_heads, head_dim)\n",
    "                    reshaped[:, :, target_head_num, :] = 0\n",
    "                    modified = reshaped.view(batch_size_tensor, seq_len, hidden_dim)\n",
    "                    \n",
    "                    return (modified,) + output[1:]\n",
    "                return ablation_hook\n",
    "            \n",
    "            # Register hook for this layer\n",
    "            attention_layer = model.model.layers[layer_idx].self_attn\n",
    "            hook = attention_layer.register_forward_hook(create_ablation_hook(head_num))\n",
    "            hooks.append((hook, layer_idx, head_num))\n",
    "        \n",
    "        print(f\"  Successfully added {len(hooks)} hooks. Running generation...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate rollouts with ALL heads ablated simultaneously\n",
    "            rollout_texts, rollout_answer_correct, rollout_answer_correct_check = generate_diverse_rollouts(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer, \n",
    "                ground_truth_answer=ground_truth_answer,\n",
    "                context=context_without,\n",
    "                num_rollouts=num_rollouts,\n",
    "                batch_size=batch_size,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                sentence_idx=sentence_idx\n",
    "            )\n",
    "            \n",
    "            # Process rollout embeddings (same as baseline)\n",
    "            rollout_sentences = [split_solution_into_chunks(rollout_text)[0] for rollout_text in rollout_texts]\n",
    "            \n",
    "            # Get all embeddings at once\n",
    "            rollout_llm_embeddings, rollout_sentence_embeddings = get_both_embeddings_batch(\n",
    "                model, sentence_model, tokenizer, rollout_sentences\n",
    "            )\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            cos_sims_llm = torch.cosine_similarity(\n",
    "                original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "            )\n",
    "            cos_sims_sent = torch.cosine_similarity(\n",
    "                original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "            )\n",
    "            \n",
    "            # Calculate basic statistics\n",
    "            unique_responses = len(set(rollout_texts))\n",
    "            \n",
    "            # Store ablation result for ALL heads ablated together\n",
    "            ablation_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"function_tags\": function_tags,\n",
    "                \n",
    "                # Multi-head ablation info\n",
    "                \"ablated_heads\": receiver_heads,  # List of all ablated heads\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"num_ablated_heads\": len(receiver_heads),\n",
    "                \n",
    "                # Context information\n",
    "                \"context_without_sentence\": context_without,\n",
    "                \"ground_truth_answer\": ground_truth_answer,\n",
    "                \n",
    "                # Rollout data - SAME AS YOUR BASELINE\n",
    "                \"num_rollouts\": num_rollouts,\n",
    "                # \"rollout_texts\": rollout_texts,  # Full rollout texts\n",
    "                \"rollout_sentences\": rollout_sentences,\n",
    "                \"rollout_answer_correct\": rollout_answer_correct,\n",
    "                \"rollout_answer_correct_check\": rollout_answer_correct_check,\n",
    "                \"unique_responses\": unique_responses,\n",
    "                \n",
    "                # Extracted boxed answers\n",
    "                \"rollout_boxed_answers\": [extract_boxed_answers(text) for text in rollout_texts],\n",
    "                \n",
    "                # Raw similarity scores\n",
    "                \"cos_sims_llm\": cos_sims_llm.cpu().tolist(),\n",
    "                \"cos_sims_sentence\": cos_sims_sent.cpu().tolist(),\n",
    "                \n",
    "                # Embedding comparison\n",
    "                \"embedding_correlation\": float(torch.corrcoef(torch.stack([cos_sims_llm, cos_sims_sent]))[0, 1]) if len(cos_sims_llm) > 1 else 0.0,\n",
    "                \n",
    "                # Metadata\n",
    "                \"generation_params\": {\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"batch_size\": batch_size\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            all_ablation_results.append(ablation_result)\n",
    "            \n",
    "            print(f\"  Successfully generated {len(rollout_texts)} rollouts with all heads ablated\")\n",
    "            print(f\"  Unique responses: {unique_responses}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del rollout_llm_embeddings, rollout_sentence_embeddings\n",
    "            del cos_sims_llm, cos_sims_sent, rollout_sentences\n",
    "            # rollout_texts will be garbage collected after saving\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error during multi-head ablation: {e}\")\n",
    "            # Store error result\n",
    "            error_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"ablated_heads\": receiver_heads,\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "            all_ablation_results.append(error_result)\n",
    "            \n",
    "        finally:\n",
    "            # ALWAYS remove all hooks\n",
    "            print(f\"  Removing {len(hooks)} hooks...\")\n",
    "            for hook, layer_idx, head_num in hooks:\n",
    "                hook.remove()\n",
    "            hooks.clear()\n",
    "        \n",
    "        # Cleanup after processing this sentence\n",
    "        del original_llm_emb, original_sent_emb\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save all results\n",
    "    results_file = problem_dir / \"sentence_multi_head_ablation_rollouts.json\"\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_ablation_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        \"problem_id\": problem_id,\n",
    "        \"total_sentences\": len(allsentences),\n",
    "        \"ablated_heads\": receiver_heads,\n",
    "        \"num_ablated_heads\": len(receiver_heads),\n",
    "        \"ablation_type\": \"multi_head_simultaneous\",\n",
    "        \"total_ablation_experiments\": len(allsentences),  # One experiment per sentence\n",
    "        \"successful_experiments\": len([r for r in all_ablation_results if \"error\" not in r]),\n",
    "        \"failed_experiments\": len([r for r in all_ablation_results if \"error\" in r]),\n",
    "        \"generation_params\": {\n",
    "            \"num_rollouts\": num_rollouts,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = problem_dir / \"multi_head_ablation_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nCompleted problem {problem_id}\")\n",
    "    print(f\"  - Total sentences: {len(allsentences)}\")\n",
    "    print(f\"  - Heads ablated simultaneously: {len(receiver_heads)}\")\n",
    "    print(f\"  - Total experiments: {len(allsentences)}\")\n",
    "    print(f\"  - Successful: {summary['successful_experiments']}\")\n",
    "    print(f\"  - Failed: {summary['failed_experiments']}\")\n",
    "    print(f\"  - Results saved to: {results_file}\")\n",
    "    \n",
    "    return all_ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c79f86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_problem_with_multi_head_ablation(\n",
    "    problem_data: Dict, \n",
    "    problem_labels: Dict,\n",
    "    model, \n",
    "    tokenizer, \n",
    "    sentence_model,\n",
    "    receiver_heads: List[tuple],  # List of (layer_idx, head_idx) tuples\n",
    "    output_dir: str = \"rollout_results_ablation\",\n",
    "    num_rollouts: int = 6,\n",
    "    batch_size: int = 6,\n",
    "    force: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single problem with receiver head ablation: loop through every sentence \n",
    "    and ablate ALL receiver heads simultaneously, storing rollouts and similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    problem_id = problem_data[\"problem_id\"]\n",
    "    problem_text_prompt = problem_data[\"problem_statement\"]\n",
    "    allsentences = problem_data[\"sentences\"]\n",
    "    ground_truth_answer = problem_data[\"answer\"]\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_path = Path(output_dir)\n",
    "    problem_dir = output_path / problem_id\n",
    "    problem_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Save all results\n",
    "    results_file = problem_dir / \"sentence_multi_head_ablation_rollouts.json\"\n",
    "    \n",
    "    # Prepare problem text for generation\n",
    "    problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "    print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} sentences\")\n",
    "    print(f\"Will ablate {len(receiver_heads)} receiver heads SIMULTANEOUSLY per sentence\")\n",
    "\n",
    "    # Check if we can resume from existing results\n",
    "    all_ablation_results = []\n",
    "    start_sentence_idx = 0\n",
    "    \n",
    "    if results_file.exists() and not force:\n",
    "        try:\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                existing_results = json.load(f)\n",
    "            \n",
    "            # Filter out failed results to find the last successful sentence\n",
    "            successful_results = [r for r in existing_results if \"error\" not in r]\n",
    "            \n",
    "            if successful_results:\n",
    "                all_ablation_results = existing_results\n",
    "                start_sentence_idx = len(successful_results)\n",
    "                print(f\"Resuming from sentence {start_sentence_idx + 1}/{len(allsentences)}\")\n",
    "                print(f\"Found {len(successful_results)} existing successful results\")\n",
    "            else:\n",
    "                print(f\"Found existing file but no successful results. Starting from beginning.\")\n",
    "                \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error reading existing results file: {e}. Starting from beginning.\")\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence_idx in range(start_sentence_idx, len(allsentences)):\n",
    "        print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} ---\")\n",
    "        print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "        \n",
    "        # Get context without current sentence (same as baseline)\n",
    "        prefix_text = allsentences[:sentence_idx]\n",
    "        context_without = problem_text + \" \" + \" \".join(prefix_text) + \"\"\n",
    "        \n",
    "        # Get function tags if available\n",
    "        function_tags = []\n",
    "        if sentence_idx < len(problem_labels.get('chunks', [])):\n",
    "            function_tags = problem_labels['chunks'][sentence_idx].get('function_tags', [])\n",
    "        \n",
    "        # Get original sentence embeddings\n",
    "        original_text = allsentences[sentence_idx]\n",
    "        original_llm_emb, original_sent_emb = get_both_embeddings(model, sentence_model, tokenizer, original_text)\n",
    "        \n",
    "        # CREATE ABLATION HOOKS FOR ALL HEADS\n",
    "        hooks = []\n",
    "        print(f\"  Adding hooks for {len(receiver_heads)} heads...\")\n",
    "        \n",
    "        for layer_idx, head_num in receiver_heads:\n",
    "            print(f\"    Adding hook for head ({layer_idx}, {head_num})\")\n",
    "            \n",
    "            # Define ablation hook for this specific head\n",
    "            def create_ablation_hook(target_head_num):\n",
    "                def ablation_hook(module, input, output):\n",
    "                    attention_output = output[0]\n",
    "                    batch_size_tensor, seq_len, hidden_dim = attention_output.shape\n",
    "                    num_heads = getattr(module, 'num_heads', getattr(module, 'num_attention_heads', 32))\n",
    "                    head_dim = hidden_dim // num_heads\n",
    "                    \n",
    "                    # Zero out the specific head\n",
    "                    reshaped = attention_output.view(batch_size_tensor, seq_len, num_heads, head_dim)\n",
    "                    reshaped[:, :, target_head_num, :] = 0\n",
    "                    modified = reshaped.view(batch_size_tensor, seq_len, hidden_dim)\n",
    "                    \n",
    "                    return (modified,) + output[1:]\n",
    "                return ablation_hook\n",
    "            \n",
    "            # Register hook for this layer\n",
    "            attention_layer = model.model.layers[layer_idx].self_attn\n",
    "            hook = attention_layer.register_forward_hook(create_ablation_hook(head_num))\n",
    "            hooks.append((hook, layer_idx, head_num))\n",
    "        \n",
    "        print(f\"  Successfully added {len(hooks)} hooks. Running generation...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate rollouts with ALL heads ablated simultaneously\n",
    "            rollout_texts, rollout_answer_correct, rollout_answer_correct_check = generate_diverse_rollouts(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer, \n",
    "                ground_truth_answer=ground_truth_answer,\n",
    "                context=context_without,\n",
    "                num_rollouts=num_rollouts,\n",
    "                batch_size=batch_size,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                sentence_idx=sentence_idx\n",
    "            )\n",
    "            \n",
    "            # Process rollout embeddings (same as baseline)\n",
    "            rollout_sentences = [split_solution_into_chunks(rollout_text)[0] for rollout_text in rollout_texts]\n",
    "            \n",
    "            # Get all embeddings at once\n",
    "            rollout_llm_embeddings, rollout_sentence_embeddings = get_both_embeddings_batch(\n",
    "                model, sentence_model, tokenizer, rollout_sentences\n",
    "            )\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            cos_sims_llm = torch.cosine_similarity(\n",
    "                original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "            )\n",
    "            cos_sims_sent = torch.cosine_similarity(\n",
    "                original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "            )\n",
    "            \n",
    "            # Calculate basic statistics\n",
    "            unique_responses = len(set(rollout_texts))\n",
    "            \n",
    "            # Store ablation result for ALL heads ablated together\n",
    "            ablation_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"function_tags\": function_tags,\n",
    "                \n",
    "                # Multi-head ablation info\n",
    "                \"ablated_heads\": receiver_heads,  # List of all ablated heads\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"num_ablated_heads\": len(receiver_heads),\n",
    "                \n",
    "                # Context information\n",
    "                \"context_without_sentence\": context_without,\n",
    "                \"ground_truth_answer\": ground_truth_answer,\n",
    "                \n",
    "                # Rollout data - SAME AS YOUR BASELINE\n",
    "                \"num_rollouts\": num_rollouts,\n",
    "                # \"rollout_texts\": rollout_texts,  # Full rollout texts\n",
    "                \"rollout_sentences\": rollout_sentences,\n",
    "                \"rollout_answer_correct\": rollout_answer_correct,\n",
    "                \"rollout_answer_correct_check\": rollout_answer_correct_check,\n",
    "                \"unique_responses\": unique_responses,\n",
    "                \n",
    "                # Extracted boxed answers\n",
    "                \"rollout_boxed_answers\": [extract_boxed_answers(text) for text in rollout_texts],\n",
    "                \n",
    "                # Raw similarity scores\n",
    "                \"cos_sims_llm\": cos_sims_llm.cpu().tolist(),\n",
    "                \"cos_sims_sentence\": cos_sims_sent.cpu().tolist(),\n",
    "                \n",
    "                # Embedding comparison\n",
    "                \"embedding_correlation\": float(torch.corrcoef(torch.stack([cos_sims_llm, cos_sims_sent]))[0, 1]) if len(cos_sims_llm) > 1 else 0.0,\n",
    "                \n",
    "                # Metadata\n",
    "                \"generation_params\": {\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"batch_size\": batch_size\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            all_ablation_results.append(ablation_result)\n",
    "            \n",
    "            print(f\"  Successfully generated {len(rollout_texts)} rollouts with all heads ablated\")\n",
    "            print(f\"  Unique responses: {unique_responses}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del rollout_llm_embeddings, rollout_sentence_embeddings\n",
    "            del cos_sims_llm, cos_sims_sent, rollout_sentences\n",
    "            # rollout_texts will be garbage collected after saving\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error during multi-head ablation: {e}\")\n",
    "            \n",
    "            # 🛡️ Emergency hook cleanup on error\n",
    "            print(f\"  Emergency hook cleanup - removing {len(hooks)} hooks...\")\n",
    "            for hook, layer_idx, head_num in hooks:\n",
    "                try:\n",
    "                    hook.remove()\n",
    "                except:\n",
    "                    pass  # Hook might already be removed\n",
    "            hooks.clear()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Store error result\n",
    "            error_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"ablated_heads\": receiver_heads,\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "            all_ablation_results.append(error_result)\n",
    "            \n",
    "        finally:\n",
    "            # ALWAYS remove all hooks\n",
    "            print(f\"  Removing {len(hooks)} hooks...\")\n",
    "            for hook, layer_idx, head_num in hooks:\n",
    "                try:\n",
    "                    hook.remove()\n",
    "                except:\n",
    "                    pass  # Might already be removed in error handling\n",
    "            hooks.clear()\n",
    "        \n",
    "        # 🔥 SAVE AFTER EVERY SENTENCE (FIXED!)\n",
    "        print(f\"  Saving results after sentence {sentence_idx + 1}...\")\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_ablation_results, f, indent=2, default=str)  # ✅ Save FULL list!\n",
    "        print(f\"  Results saved to: {results_file}\")\n",
    "        \n",
    "        # Cleanup after processing this sentence\n",
    "        del original_llm_emb, original_sent_emb\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Final save (this is now redundant but harmless)\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_ablation_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        \"problem_id\": problem_id,\n",
    "        \"total_sentences\": len(allsentences),\n",
    "        \"ablated_heads\": receiver_heads,\n",
    "        \"num_ablated_heads\": len(receiver_heads),\n",
    "        \"ablation_type\": \"multi_head_simultaneous\",\n",
    "        \"total_ablation_experiments\": len(allsentences),  # One experiment per sentence\n",
    "        \"successful_experiments\": len([r for r in all_ablation_results if \"error\" not in r]),\n",
    "        \"failed_experiments\": len([r for r in all_ablation_results if \"error\" in r]),\n",
    "        \"generation_params\": {\n",
    "            \"num_rollouts\": num_rollouts,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = problem_dir / \"multi_head_ablation_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nCompleted problem {problem_id}\")\n",
    "    print(f\"  - Total sentences: {len(allsentences)}\")\n",
    "    print(f\"  - Heads ablated simultaneously: {len(receiver_heads)}\")\n",
    "    print(f\"  - Total experiments: {len(allsentences)}\")\n",
    "    print(f\"  - Successful: {summary['successful_experiments']}\")\n",
    "    print(f\"  - Failed: {summary['failed_experiments']}\")\n",
    "    print(f\"  - Results saved to: {results_file}\")\n",
    "    \n",
    "    return all_ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69efbf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "952a7794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing problem problem_6481 with 187 sentences\n",
      "Will ablate 20 receiver heads SIMULTANEOUSLY per sentence\n",
      "Resuming from sentence 184/187\n",
      "Found 183 existing successful results\n",
      "\n",
      "--- Processing sentence 184/187 ---\n",
      "Sentence: So, that seems consistent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chriskino/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Adding hooks for 20 heads...\n",
      "    Adding hook for head (1, 19)\n",
      "    Adding hook for head (1, 22)\n",
      "    Adding hook for head (1, 29)\n",
      "    Adding hook for head (2, 1)\n",
      "    Adding hook for head (5, 18)\n",
      "    Adding hook for head (5, 31)\n",
      "    Adding hook for head (6, 17)\n",
      "    Adding hook for head (7, 4)\n",
      "    Adding hook for head (8, 29)\n",
      "    Adding hook for head (12, 23)\n",
      "    Adding hook for head (13, 31)\n",
      "    Adding hook for head (14, 3)\n",
      "    Adding hook for head (14, 9)\n",
      "    Adding hook for head (14, 28)\n",
      "    Adding hook for head (15, 21)\n",
      "    Adding hook for head (17, 19)\n",
      "    Adding hook for head (17, 25)\n",
      "    Adding hook for head (26, 27)\n",
      "    Adding hook for head (27, 0)\n",
      "    Adding hook for head (28, 23)\n",
      "  Successfully added 20 hooks. Running generation...\n",
      "  Successfully generated 6 rollouts with all heads ablated\n",
      "  Unique responses: 6\n",
      "  Removing 20 hooks...\n",
      "  Saving results after sentence 184...\n",
      "  Results saved to: random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n",
      "\n",
      "--- Processing sentence 185/187 ---\n",
      "Sentence: Therefore, I think my initial calculation is correct.\n",
      "  Adding hooks for 20 heads...\n",
      "    Adding hook for head (1, 19)\n",
      "    Adding hook for head (1, 22)\n",
      "    Adding hook for head (1, 29)\n",
      "    Adding hook for head (2, 1)\n",
      "    Adding hook for head (5, 18)\n",
      "    Adding hook for head (5, 31)\n",
      "    Adding hook for head (6, 17)\n",
      "    Adding hook for head (7, 4)\n",
      "    Adding hook for head (8, 29)\n",
      "    Adding hook for head (12, 23)\n",
      "    Adding hook for head (13, 31)\n",
      "    Adding hook for head (14, 3)\n",
      "    Adding hook for head (14, 9)\n",
      "    Adding hook for head (14, 28)\n",
      "    Adding hook for head (15, 21)\n",
      "    Adding hook for head (17, 19)\n",
      "    Adding hook for head (17, 25)\n",
      "    Adding hook for head (26, 27)\n",
      "    Adding hook for head (27, 0)\n",
      "    Adding hook for head (28, 23)\n",
      "  Successfully added 20 hooks. Running generation...\n",
      "  Successfully generated 6 rollouts with all heads ablated\n",
      "  Unique responses: 6\n",
      "  Removing 20 hooks...\n",
      "  Saving results after sentence 185...\n",
      "  Results saved to: random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n",
      "\n",
      "--- Processing sentence 186/187 ---\n",
      "Sentence: Thus, the length of the boundary is approximately 30.8 units.\n",
      "  Adding hooks for 20 heads...\n",
      "    Adding hook for head (1, 19)\n",
      "    Adding hook for head (1, 22)\n",
      "    Adding hook for head (1, 29)\n",
      "    Adding hook for head (2, 1)\n",
      "    Adding hook for head (5, 18)\n",
      "    Adding hook for head (5, 31)\n",
      "    Adding hook for head (6, 17)\n",
      "    Adding hook for head (7, 4)\n",
      "    Adding hook for head (8, 29)\n",
      "    Adding hook for head (12, 23)\n",
      "    Adding hook for head (13, 31)\n",
      "    Adding hook for head (14, 3)\n",
      "    Adding hook for head (14, 9)\n",
      "    Adding hook for head (14, 28)\n",
      "    Adding hook for head (15, 21)\n",
      "    Adding hook for head (17, 19)\n",
      "    Adding hook for head (17, 25)\n",
      "    Adding hook for head (26, 27)\n",
      "    Adding hook for head (27, 0)\n",
      "    Adding hook for head (28, 23)\n",
      "  Successfully added 20 hooks. Running generation...\n",
      "  Successfully generated 6 rollouts with all heads ablated\n",
      "  Unique responses: 6\n",
      "  Removing 20 hooks...\n",
      "  Saving results after sentence 186...\n",
      "  Results saved to: random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n",
      "\n",
      "--- Processing sentence 187/187 ---\n",
      "Sentence: **Final Answer**\n",
      "The length of the boundary of the bolded figure is \\boxed{30.8}.\n",
      "  Adding hooks for 20 heads...\n",
      "    Adding hook for head (1, 19)\n",
      "    Adding hook for head (1, 22)\n",
      "    Adding hook for head (1, 29)\n",
      "    Adding hook for head (2, 1)\n",
      "    Adding hook for head (5, 18)\n",
      "    Adding hook for head (5, 31)\n",
      "    Adding hook for head (6, 17)\n",
      "    Adding hook for head (7, 4)\n",
      "    Adding hook for head (8, 29)\n",
      "    Adding hook for head (12, 23)\n",
      "    Adding hook for head (13, 31)\n",
      "    Adding hook for head (14, 3)\n",
      "    Adding hook for head (14, 9)\n",
      "    Adding hook for head (14, 28)\n",
      "    Adding hook for head (15, 21)\n",
      "    Adding hook for head (17, 19)\n",
      "    Adding hook for head (17, 25)\n",
      "    Adding hook for head (26, 27)\n",
      "    Adding hook for head (27, 0)\n",
      "    Adding hook for head (28, 23)\n",
      "  Successfully added 20 hooks. Running generation...\n",
      "  Successfully generated 6 rollouts with all heads ablated\n",
      "  Unique responses: 6\n",
      "  Removing 20 hooks...\n",
      "  Saving results after sentence 187...\n",
      "  Results saved to: random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n",
      "\n",
      "Completed problem problem_6481\n",
      "  - Total sentences: 187\n",
      "  - Heads ablated simultaneously: 20\n",
      "  - Total experiments: 187\n",
      "  - Successful: 187\n",
      "  - Failed: 0\n",
      "  - Results saved to: random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n"
     ]
    }
   ],
   "source": [
    "results = process_single_problem_with_multi_head_ablation(\n",
    "    problem_data=all_prompt[0],\n",
    "    problem_labels=all_labels[0], \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    sentence_model=sentence_model,\n",
    "    receiver_heads=random_receiver_heads,  # Your 20 heads\n",
    "    output_dir=\"random_rollout_results_ablation\",\n",
    "    num_rollouts=6,\n",
    "    batch_size=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2864b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing problem problem_6481 with 187 sentences\n",
      "Will ablate 20 receiver heads SIMULTANEOUSLY per sentence\n",
      "Resuming from sentence 187/187\n",
      "Found 186 existing successful results\n",
      "\n",
      "--- Processing sentence 187/187 ---\n",
      "Sentence: **Final Answer**\n",
      "The length of the boundary of the bolded figure is \\boxed{30.8}.\n",
      "  Adding hooks for 20 heads...\n",
      "    Adding hook for head (1, 19)\n",
      "    Adding hook for head (1, 22)\n",
      "    Adding hook for head (1, 29)\n",
      "    Adding hook for head (2, 1)\n",
      "    Adding hook for head (5, 18)\n",
      "    Adding hook for head (5, 31)\n",
      "    Adding hook for head (6, 17)\n",
      "    Adding hook for head (7, 4)\n",
      "    Adding hook for head (8, 29)\n",
      "    Adding hook for head (12, 23)\n",
      "    Adding hook for head (13, 31)\n",
      "    Adding hook for head (14, 3)\n",
      "    Adding hook for head (14, 9)\n",
      "    Adding hook for head (14, 28)\n",
      "    Adding hook for head (15, 21)\n",
      "    Adding hook for head (17, 19)\n",
      "    Adding hook for head (17, 25)\n",
      "    Adding hook for head (26, 27)\n",
      "    Adding hook for head (27, 0)\n",
      "    Adding hook for head (28, 23)\n",
      "  Successfully added 20 hooks. Running generation...\n",
      "  Successfully generated 6 rollouts with all heads ablated\n",
      "  Unique responses: 6\n",
      "  Removing 20 hooks...\n",
      "  Saving results after sentence 187...\n",
      "  Results saved to: 2_random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n",
      "\n",
      "Completed problem problem_6481\n",
      "  - Total sentences: 187\n",
      "  - Heads ablated simultaneously: 20\n",
      "  - Total experiments: 187\n",
      "  - Successful: 187\n",
      "  - Failed: 0\n",
      "  - Results saved to: 2_random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n"
     ]
    }
   ],
   "source": [
    "results = process_single_problem_with_multi_head_ablation(\n",
    "    problem_data=all_prompt[0],\n",
    "    problem_labels=all_labels[0], \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    sentence_model=sentence_model,\n",
    "    receiver_heads=random_receiver_heads,  # Your 20 heads\n",
    "    output_dir=\"2_random_rollout_results_ablation\",\n",
    "    num_rollouts=6,\n",
    "    batch_size=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "218c1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_list(merged_rollouts):\n",
    "    \"\"\"Simple validation function to check if the merged rollouts are valid.\"\"\"\n",
    "    if not merged_rollouts:\n",
    "        print(\"Warning: Empty rollouts list\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✓ Total sentences: {len(merged_rollouts)}\")\n",
    "    \n",
    "    # Count successful vs failed\n",
    "    successful = [r for r in merged_rollouts if 'error' not in r]\n",
    "    failed = [r for r in merged_rollouts if 'error' in r]\n",
    "    \n",
    "    print(f\"✓ Successful sentences: {len(successful)}\")\n",
    "    print(f\"✓ Failed sentences: {len(failed)}\")\n",
    "    \n",
    "    if successful:\n",
    "        total_rollouts = sum(r.get('num_rollouts', 0) for r in successful)\n",
    "        print(f\"✓ Total rollouts: {total_rollouts}\")\n",
    "        \n",
    "        # Check if we have the expected data structure\n",
    "        sample = successful[0]\n",
    "        required_fields = ['rollout_sentences', 'cos_sims_llm', 'cos_sims_sentence']\n",
    "        missing_fields = [field for field in required_fields if field not in sample]\n",
    "        \n",
    "        if missing_fields:\n",
    "            print(f\"⚠️  Missing fields in data: {missing_fields}\")\n",
    "        else:\n",
    "            print(\"✓ All required fields present\")\n",
    "    \n",
    "    return len(failed) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c4a09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def clean_ablation_rollout_file(rollout_file_path: str, backup: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Remove duplicate sentence_idx entries and any entries with errors from a rollout file.\n",
    "    Returns the number of removed entries.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import shutil\n",
    "\n",
    "    rollout_file = Path(rollout_file_path)\n",
    "    if not rollout_file.exists():\n",
    "        print(f\"File not found: {rollout_file}\")\n",
    "        return 0\n",
    "\n",
    "    with open(rollout_file, 'r', encoding='utf-8') as f:\n",
    "        rollouts = json.load(f)\n",
    "\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    removed = 0\n",
    "    for entry in rollouts:\n",
    "        idx = entry.get(\"sentence_idx\")\n",
    "        if \"error\" in entry:\n",
    "            removed += 1\n",
    "            continue\n",
    "        if idx in seen:\n",
    "            removed += 1\n",
    "            continue\n",
    "        seen.add(idx)\n",
    "        cleaned.append(entry)\n",
    "\n",
    "    # Sort by sentence_idx\n",
    "    cleaned.sort(key=lambda x: x.get(\"sentence_idx\", -1))\n",
    "\n",
    "    if removed > 0 or len(cleaned) != len(rollouts):\n",
    "        if backup:\n",
    "            backup_file = rollout_file.with_suffix('.bak.json')\n",
    "            shutil.copy2(rollout_file, backup_file)\n",
    "            print(f\"Backup saved: {backup_file}\")\n",
    "        with open(rollout_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned, f, indent=2, default=str)\n",
    "        print(f\"Cleaned {removed} error/duplicate entries in {rollout_file.name}. Now {len(cleaned)} entries.\")\n",
    "    else:\n",
    "        print(f\"No errors or duplicates found in {rollout_file.name}.\")\n",
    "\n",
    "    return removed\n",
    "\n",
    "# Example usage:\n",
    "# clean_ablation_rollout_file(\"random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\")\n",
    "# clean_ablation_rollout_file(\"2_random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\")\n",
    "\n",
    "# Example usage:\n",
    "# clean_ablation_rollout_file(\"random_rollout_results_ablation/problem_XXX/sentence_multi_head_ablation_rollouts.json\")\n",
    "# clean_ablation_rollout_file(\"2_random_rollout_results_ablation/problem_XXX/sentence_multi_head_ablation_rollouts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08bd85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_ablation_rollout_file(\"random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\")\n",
    "# clean_ablation_rollout_file(\"2_random_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04288f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected problem_id: problem_6481\n",
      "Additional ablation rollouts not found: 2_rollout_results_ablation/problem_6481/sentence_multi_head_ablation_rollouts.json\n"
     ]
    }
   ],
   "source": [
    "def merge_ablation_rollouts_from_two_directories(\n",
    "    original_dir: str = \"random_rollout_results_ablation/\",\n",
    "    additional_dir: str = \"2_random_rollout_results_ablation/\", \n",
    "    problem_id: str = None,\n",
    "    output_dir: str = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Merge ABLATION rollouts from two different directories for the same problem.\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = original_dir\n",
    "    \n",
    "    # Auto-detect problem_id if not provided\n",
    "    if problem_id is None:\n",
    "        original_path = Path(original_dir)\n",
    "        problem_dirs = [d.name for d in original_path.iterdir() if d.is_dir()]\n",
    "        if problem_dirs:\n",
    "            problem_id = problem_dirs[0]\n",
    "            print(f\"Auto-detected problem_id: {problem_id}\")\n",
    "        else:\n",
    "            print(f\"No problem directories found in {original_dir}\")\n",
    "            return []\n",
    "    \n",
    "    # 🔧 FIX: Use correct ablation file names\n",
    "    original_file = Path(original_dir) / problem_id / \"sentence_multi_head_ablation_rollouts.json\"\n",
    "    additional_file = Path(additional_dir) / problem_id / \"sentence_multi_head_ablation_rollouts.json\"\n",
    "    \n",
    "    if not original_file.exists():\n",
    "        print(f\"Original ablation rollouts not found: {original_file}\")\n",
    "        return []\n",
    "    \n",
    "    if not additional_file.exists():\n",
    "        print(f\"Additional ablation rollouts not found: {additional_file}\")\n",
    "        return []\n",
    "    \n",
    "    # Load both datasets\n",
    "    with open(original_file, 'r', encoding='utf-8') as f:\n",
    "        original_rollouts = json.load(f)\n",
    "    \n",
    "    with open(additional_file, 'r', encoding='utf-8') as f:\n",
    "        additional_rollouts = json.load(f)\n",
    "    \n",
    "    print(f\"Original ablation rollouts: {len(original_rollouts)} sentences\")\n",
    "    print(f\"Additional ablation rollouts: {len(additional_rollouts)} sentences\")\n",
    "    \n",
    "    # 🔧 ADD: The actual merge logic\n",
    "    if len(original_rollouts) != len(additional_rollouts):\n",
    "        print(f\"⚠️  Warning: Different number of sentences in datasets!\")\n",
    "        print(f\"  Original: {len(original_rollouts)}, Additional: {len(additional_rollouts)}\")\n",
    "    \n",
    "    # Create backup before merging\n",
    "    output_path = Path(output_dir) / problem_id\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    backup_file = output_path / f\"sentence_multi_head_ablation_rollouts_backup.json\"\n",
    "    if original_file.exists():\n",
    "        import shutil\n",
    "        shutil.copy2(original_file, backup_file)\n",
    "        print(f\"✅ Backup created: {backup_file}\")\n",
    "    \n",
    "    # Merge sentence by sentence\n",
    "    merged_rollouts = []\n",
    "    \n",
    "    for i in range(min(len(original_rollouts), len(additional_rollouts))):\n",
    "        original_sentence = original_rollouts[i]\n",
    "        additional_sentence = additional_rollouts[i]\n",
    "        \n",
    "        # Skip if either sentence failed\n",
    "        if \"error\" in original_sentence or \"error\" in additional_sentence:\n",
    "            print(f\"Sentence {i}: Skipping due to errors\")\n",
    "            merged_rollouts.append(original_sentence)  # Keep original\n",
    "            continue\n",
    "        \n",
    "        # Verify they're the same sentence\n",
    "        if original_sentence.get(\"sentence_idx\") != additional_sentence.get(\"sentence_idx\"):\n",
    "            print(f\"⚠️  Sentence index mismatch at position {i}\")\n",
    "            merged_rollouts.append(original_sentence)\n",
    "            continue\n",
    "        \n",
    "        # Merge rollout data\n",
    "        merged_sentence = original_sentence.copy()\n",
    "        \n",
    "        # Combine rollout lists\n",
    "        merged_sentence[\"rollout_sentences\"] = (\n",
    "            original_sentence[\"rollout_sentences\"] + \n",
    "            additional_sentence[\"rollout_sentences\"]\n",
    "        )\n",
    "        merged_sentence[\"rollout_answer_correct\"] = (\n",
    "            original_sentence[\"rollout_answer_correct\"] + \n",
    "            additional_sentence[\"rollout_answer_correct\"]\n",
    "        )\n",
    "        merged_sentence[\"rollout_answer_correct_check\"] = (\n",
    "            original_sentence[\"rollout_answer_correct_check\"] + \n",
    "            additional_sentence[\"rollout_answer_correct_check\"]\n",
    "        )\n",
    "        merged_sentence[\"rollout_boxed_answers\"] = (\n",
    "            original_sentence[\"rollout_boxed_answers\"] + \n",
    "            additional_sentence[\"rollout_boxed_answers\"]\n",
    "        )\n",
    "        merged_sentence[\"cos_sims_llm\"] = (\n",
    "            original_sentence[\"cos_sims_llm\"] + \n",
    "            additional_sentence[\"cos_sims_llm\"]\n",
    "        )\n",
    "        merged_sentence[\"cos_sims_sentence\"] = (\n",
    "            original_sentence[\"cos_sims_sentence\"] + \n",
    "            additional_sentence[\"cos_sims_sentence\"]\n",
    "        )\n",
    "        \n",
    "        # Update counts\n",
    "        merged_sentence[\"num_rollouts\"] = (\n",
    "            original_sentence[\"num_rollouts\"] + \n",
    "            additional_sentence[\"num_rollouts\"]\n",
    "        )\n",
    "        merged_sentence[\"unique_responses\"] = len(set(merged_sentence[\"rollout_sentences\"]))\n",
    "        \n",
    "        merged_rollouts.append(merged_sentence)\n",
    "        \n",
    "        print(f\"Sentence {i}: Merged {original_sentence['num_rollouts']} + {additional_sentence['num_rollouts']} = {merged_sentence['num_rollouts']} rollouts\")\n",
    "    \n",
    "    # 🔧 FIX: Save to correct ablation file name with proper output_path\n",
    "    merged_file = output_path / \"sentence_multi_head_ablation_rollouts.json\"\n",
    "    with open(merged_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_rollouts, f, indent=2, default=str)\n",
    "    \n",
    "    # Save merge summary\n",
    "    merge_summary = {\n",
    "        \"merge_timestamp\": str(datetime.now()),\n",
    "        \"original_dir\": original_dir,\n",
    "        \"additional_dir\": additional_dir,\n",
    "        \"problem_id\": problem_id,\n",
    "        \"original_sentences\": len(original_rollouts),\n",
    "        \"additional_sentences\": len(additional_rollouts),\n",
    "        \"merged_sentences\": len(merged_rollouts),\n",
    "        \"total_rollouts\": sum(s.get(\"num_rollouts\", 0) for s in merged_rollouts if \"error\" not in s),\n",
    "        \"backup_created\": str(backup_file)\n",
    "    }\n",
    "    \n",
    "    summary_file = output_path / \"merge_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merge_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ Merge completed successfully!\")\n",
    "    print(f\"📁 Merged file: {merged_file}\")\n",
    "    print(f\"📄 Summary: {summary_file}\")\n",
    "    print(f\"💾 Backup: {backup_file}\")\n",
    "    \n",
    "    return merged_rollouts\n",
    "\n",
    "# Add missing import\n",
    "from datetime import datetime\n",
    "\n",
    "# 🔧 FIX: Call with correct function name and parameters\n",
    "merged_rollouts = merge_ablation_rollouts_from_two_directories(\n",
    "    original_dir=\"random_rollout_results_ablation/\",\n",
    "    additional_dir=\"2_random_rollout_results_ablation/\", \n",
    "    problem_id=None,  # Will auto-detect\n",
    "    output_dir=\"random_rollout_results_ablation/\"  # Save back to original directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3f0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160cac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafb4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
