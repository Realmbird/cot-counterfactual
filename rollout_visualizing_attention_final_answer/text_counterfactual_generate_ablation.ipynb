{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3daa832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reciever_heads = [(29, 24),\n",
    " (17, 0),\n",
    " (24, 7),\n",
    " (25, 22),\n",
    " (23, 8),\n",
    " (18, 12),\n",
    " (23, 23),\n",
    " (21, 4),\n",
    " (19, 17),\n",
    " (18, 14),\n",
    " (30, 17),\n",
    " (19, 27),\n",
    " (28, 22),\n",
    " (1, 17),\n",
    " (27, 1),\n",
    " (24, 1),\n",
    " (26, 10),\n",
    " (26, 24),\n",
    " (1, 16),\n",
    " (24, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0d2693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found problem directory: ../math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\n",
      "Found problem directory: ['problem_6481', 'problem_4682', 'problem_3360', 'problem_4605', 'problem_2236', 'problem_1591', 'problem_4164', 'problem_2189', 'problem_2238', 'problem_3935', 'problem_6596', 'problem_3550', 'problem_2870', 'problem_4019', 'problem_2050', 'problem_6998', 'problem_3916', 'problem_2137', 'problem_3448', 'problem_330']\n",
      "Successfully loaded data for 20 problems.\n",
      "Found problem directory: ../math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\n",
      "Found problem directory: ['problem_4019', 'problem_2870', 'problem_3550', 'problem_3935', 'problem_6596', 'problem_2238', 'problem_2189', 'problem_4164', 'problem_1591', 'problem_2236', 'problem_4605', 'problem_3360', 'problem_6481', 'problem_4682', 'problem_3448', 'problem_2137', 'problem_330', 'problem_6998', 'problem_3916', 'problem_2050']\n",
      "Successfully loaded data for 20 problems.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "def process_problem_data(base_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Iterates through all problem directories, extracts problem statements\n",
    "\n",
    "    and sentences from `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "        base_path (str): The path to the directory containing all the problems\n",
    "\n",
    "                         (e.g., 'math-rollouts/.../correct_base_solution').\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        list: A list of dictionaries, where each dictionary contains the problem\n",
    "\n",
    "              and all sentences for a given problem directory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_problem_data = []\n",
    "\n",
    "\n",
    "    # Check if the base path exists\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "    print(f\"Found problem directory: {base_path}\")\n",
    "\n",
    "\n",
    "    # List all entries in the base directory\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "    print(f\"Found problem directory: {problem_dirs}\")\n",
    "\n",
    "\n",
    "    if not problem_dirs:\n",
    "\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "\n",
    "    # Iterate through each problem directory (e.g., problem_330, problem_1591)\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "\n",
    "       \n",
    "\n",
    "        # Define the file paths for the problem and chunks\n",
    "\n",
    "        problem_file = os.path.join(problem_path, \"problem.json\")\n",
    "\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "\n",
    "       \n",
    "\n",
    "        problem_text = \"\"\n",
    "\n",
    "        allsentences = []\n",
    "\n",
    "       \n",
    "\n",
    "        # Load the problem statement\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(problem_file, 'r') as f:\n",
    "\n",
    "                problem_data = json.load(f)\n",
    "\n",
    "                problem_text = problem_data.get(\"problem\", \"\")\n",
    "                problem_answer = problem_data.get(\"gt_answer\", \"\")\n",
    "                \n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load problem.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Load all sentences from chunks_labeled.json\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(chunks_file, 'r') as f:\n",
    "\n",
    "                chunks_data = json.load(f)\n",
    "\n",
    "                allsentences = [chunk[\"chunk\"] for chunk in chunks_data]\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load chunks_labeled.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Create a dictionary to store the extracted data\n",
    "\n",
    "        problem_info = {\n",
    "\n",
    "            \"problem_id\": problem_name,\n",
    "\n",
    "            \"problem_statement\": problem_text,\n",
    "\n",
    "            \"sentences\": allsentences,\n",
    "            \"answer\": problem_answer\n",
    "\n",
    "        }\n",
    "\n",
    "        all_problem_data.append(problem_info)\n",
    "\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"../math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "correct_all_prompt = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "# Now, `all_data` is a list of dictionaries. You can iterate through it.\n",
    "\n",
    "print(f\"Successfully loaded data for {len(correct_all_prompt)} problems.\")\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"../math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "incorrect_all_prompt = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "print(f\"Successfully loaded data for {len(incorrect_all_prompt)} problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae63cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompt = correct_all_prompt[:2] + incorrect_all_prompt[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9972b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 problems with selected chunk fields.\n",
      "Loaded 20 problems with selected chunk fields.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def process_problem_labels_minimal(base_path):\n",
    "    \"\"\"\n",
    "    Iterates through all problem directories, extracts selected fields from each chunk\n",
    "    in `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The path to the directory containing all the problems.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each with problem_id and a list of selected chunk fields.\n",
    "    \"\"\"\n",
    "    all_problem_data = []\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "        return all_problem_data\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    if not problem_dirs:\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "        return all_problem_data\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "        if not os.path.isfile(chunks_file):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(chunks_file, \"r\") as f:\n",
    "                chunks_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {problem_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract only the fields you care about from each chunk\n",
    "        selected_chunks = []\n",
    "        for chunk in chunks_data:\n",
    "            selected = {\n",
    "                \"function_tags\": chunk.get(\"function_tags\"),\n",
    "                \"chunk\": chunk.get(\"chunk\"),\n",
    "                \"accuracy\": chunk.get(\"accuracy\"),\n",
    "                \"resampling_importance_accuracy\": chunk.get(\"resampling_importance_accuracy\"),\n",
    "                \"resampling_importance_kl\": chunk.get(\"resampling_importance_kl\"),\n",
    "                \"counterfactual_importance_accuracy\": chunk.get(\"counterfactual_importance_accuracy\"),\n",
    "                \"counterfactual_importance_kl\": chunk.get(\"counterfactual_importance_kl\"),\n",
    "                \n",
    "                \"summary\": chunk.get(\"summary\"),\n",
    "            }\n",
    "            selected_chunks.append(selected)\n",
    "\n",
    "        all_problem_data.append({\n",
    "            \"problem_id\": problem_name,\n",
    "            \"chunks\": selected_chunks\n",
    "        })\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "# Example usage:\n",
    "base_problem_dir = \"../math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "correct_all_labels = process_problem_labels_minimal(base_problem_dir)\n",
    "print(f\"Loaded {len(correct_all_labels)} problems with selected chunk fields.\")\n",
    "base_problem_dir = \"../math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "incorrect_all_labels = process_problem_labels_minimal(base_problem_dir)\n",
    "print(f\"Loaded {len(incorrect_all_labels)} problems with selected chunk fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6abd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = correct_all_labels[:2] + incorrect_all_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d445572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" # Or any other suitable model\n",
    "\n",
    "mname = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84fb8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens_dict = {'additional_special_tokens': ['</think>']}\n",
    "# tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# # Get the token ID for </think> for use in generation\n",
    "# think_end_token_id = tokenizer.convert_tokens_to_ids('</think>')\n",
    "# stop_token_list = [tokenizer.eos_token_id, think_end_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5221844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5339e9e64864683b19cc3a0d368b8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781c73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize both embedding methods\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb846328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want do a loop through all the chunks append it to the current text set it through multiple rollouts and measure counterfactual importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761ccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55308955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_answer(text: str, ground_truth_answer: str):\n",
    "    \"\"\"\n",
    "    Extract answers and check if they match the ground truth.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract boxed answers from\n",
    "        ground_truth_answer: The correct answer to compare against\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list of answers, True if any answer matches ground truth)\n",
    "    \"\"\"\n",
    "    return ground_truth_answer in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac3da485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(model, inputs):\n",
    "    \"\"\"Get probability distribution from model.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits[0, -1, :]  # Last token logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8caf44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, inputs, layer=-1):\n",
    "    \"\"\"Get hidden state embeddings from a specific layer.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # Get embeddings and convert to float32 on GPU\n",
    "        embeddings = outputs.hidden_states[layer][0, -1, :].float()  # Convert to float32\n",
    "    del outputs\n",
    "    return embeddings  # Keep on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bc10cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence_model, inputs, layer=-1):\n",
    "    \"\"\"Get sentence embeddings using SentenceTransformer.\"\"\"\n",
    "    embedding = sentence_model.encode(inputs, convert_to_tensor=True)\n",
    "    return embedding.float()  # Ensure float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c331e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embeddings(llm_model, sentence_model, tokenizer, text):\n",
    "    \"\"\"Get both LLM and sentence transformer embeddings for comparison.\"\"\"\n",
    "    \n",
    "    # LLM embedding (your current method)\n",
    "    llm_inputs = tokenizer(text, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    llm_embedding = get_embeddings(llm_model, llm_inputs)\n",
    "    \n",
    "    # Sentence transformer embedding\n",
    "    sent_embedding = get_sentence_embeddings(sentence_model, text)\n",
    "\n",
    "    # MOVE TO CPU IMMEDIATELY to free GPU memory\n",
    "    llm_embedding_cpu = llm_embedding.cpu()\n",
    "    sent_embedding_cpu = sent_embedding.cpu()\n",
    "    \n",
    "    # Clean up GPU tensors\n",
    "    del llm_embedding, sent_embedding\n",
    "    return llm_embedding_cpu, sent_embedding_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d46c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embeddings_batch(llm_model, sentence_model, tokenizer, texts):\n",
    "    \"\"\"Get both LLM and sentence transformer embeddings for multiple texts in batches.\"\"\"\n",
    "    #texts list\n",
    "    \n",
    "    # Batch LLM embeddings\n",
    "    llm_inputs = tokenizer(texts, return_tensors=\"pt\", max_length=2048, truncation=True, padding=True)\n",
    "    device = next(llm_model.parameters()).device\n",
    "    llm_inputs = {k: v.to(device) for k, v in llm_inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model(**llm_inputs, output_hidden_states=True)\n",
    "        # Get last token embeddings for each sequence\n",
    "        llm_embeddings = outputs.hidden_states[-1][:, -1, :].float().cpu()\n",
    "    \n",
    "    # Batch sentence transformer embeddings\n",
    "    sent_embeddings = sentence_model.encode(texts, convert_to_tensor=True, batch_size=32).float().cpu()\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del outputs, llm_inputs\n",
    "    \n",
    "    return llm_embeddings, sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4cd4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0eb2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ecb2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_answers(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract answers enclosed in \\boxed{} from the text with improved handling\n",
    "    of nested braces and complex LaTeX expressions.\n",
    "\n",
    "    Args:\n",
    "        text: The text to extract boxed answers from\n",
    "\n",
    "    Returns:\n",
    "        List of extracted boxed answers\n",
    "    \"\"\"\n",
    "    # Find all occurrences of \\boxed{\n",
    "    boxed_starts = [m.start() for m in re.finditer(r\"\\\\boxed\\{\", text)]\n",
    "\n",
    "    if not boxed_starts:\n",
    "        return [\"\"]\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    for start_idx in boxed_starts:\n",
    "        # Start after \\boxed{\n",
    "        idx = start_idx + 7\n",
    "        brace_count = 1  # We've already opened one brace\n",
    "        answer = \"\"\n",
    "\n",
    "        # Parse until we find the matching closing brace\n",
    "        while idx < len(text) and brace_count > 0:\n",
    "            char = text[idx]\n",
    "\n",
    "            if char == \"{\":\n",
    "                brace_count += 1\n",
    "            elif char == \"}\":\n",
    "                brace_count -= 1\n",
    "\n",
    "                # Skip the closing brace of \\boxed{}\n",
    "                if brace_count == 0:\n",
    "                    break\n",
    "\n",
    "            if brace_count > 0:  # Only add if we're still inside the boxed content\n",
    "                answer += char\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        if answer:\n",
    "            answers.append(answer)\n",
    "\n",
    "    return answers if answers else [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0335a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(answer: str, use_sympy: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Get the final normalized and cleaned version of an answer.\n",
    "    This function combines all normalization steps used in check_answer.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to normalize\n",
    "        use_sympy: Whether to use sympy to normalize the answer\n",
    "\n",
    "    Returns:\n",
    "        The normalized answer string\n",
    "    \"\"\"\n",
    "    # First apply basic LaTeX normalization\n",
    "    normalized = normalize_latex(answer)\n",
    "\n",
    "    # Also prepare the answer for sympy if applicable\n",
    "    if use_sympy:\n",
    "        try:\n",
    "            sympy_ready = prepare_latex_for_sympy(answer)\n",
    "            if sympy_ready != normalized and len(sympy_ready) > 0:\n",
    "                return sympy_ready\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a67cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_equivalent(answer0, answer1):\n",
    "    \"\"\"\n",
    "    Check if two LaTeX expressions are mathematically equivalent using SymPy.\n",
    "\n",
    "    Args:\n",
    "        answer0: First LaTeX expression\n",
    "        answer1: Second LaTeX expression\n",
    "\n",
    "    Returns:\n",
    "        True if expressions are mathematically equivalent, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sympy.parsing.latex import parse_latex\n",
    "        import sympy\n",
    "\n",
    "        # Clean up the LaTeX expressions for parsing\n",
    "        answer0 = prepare_latex_for_sympy(answer0)\n",
    "        answer1 = prepare_latex_for_sympy(answer1)\n",
    "\n",
    "        # Parse the LaTeX expressions\n",
    "        expr1 = parse_latex(answer0)\n",
    "        expr2 = parse_latex(answer1)\n",
    "\n",
    "        # Check if they are mathematically identical\n",
    "        equals = expr1.equals(expr2)\n",
    "        # print(f\"First: {answer0}, Second: {answer1}: equals={equals}\")\n",
    "        return equals\n",
    "    except Exception as e:\n",
    "        # print(f\"Error comparing expressions: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def prepare_latex_for_sympy(latex_str):\n",
    "    \"\"\"\n",
    "    Prepare a LaTeX string for SymPy parsing by removing unsupported commands\n",
    "    and simplifying the expression.\n",
    "    \"\"\"\n",
    "    if not isinstance(latex_str, str):\n",
    "        return str(latex_str)\n",
    "\n",
    "    # Remove \\boxed{} command\n",
    "    latex_str = re.sub(r\"\\\\boxed\\{(.*?)\\}\", r\"\\1\", latex_str)\n",
    "\n",
    "    # Replace common LaTeX commands that SymPy doesn't support\n",
    "    replacements = {\n",
    "        r\"\\\\dfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\tfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\cdot\": r\"*\",\n",
    "        r\"\\\\times\": r\"*\",\n",
    "        r\"\\\\div\": r\"/\",\n",
    "        r\"\\\\left\": r\"\",\n",
    "        r\"\\\\right\": r\"\",\n",
    "        r\"\\\\textbf\": r\"\",\n",
    "        r\"\\\\text\": r\"\",\n",
    "        r\"\\\\mathrm\": r\"\",\n",
    "        r\"\\\\!\": r\"\",\n",
    "        r\",\": r\"\",\n",
    "    }\n",
    "\n",
    "    for old, new in replacements.items():\n",
    "        latex_str = re.sub(old, new, latex_str)\n",
    "\n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67b1b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_latex(latex_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize LaTeX string by applying various transformations.\n",
    "\n",
    "    Args:\n",
    "        latex_str: The LaTeX string to normalize\n",
    "\n",
    "    Returns:\n",
    "        Normalized LaTeX string\n",
    "    \"\"\"\n",
    "    normalized = latex_str.strip().lower()\n",
    "\n",
    "    # Replace different fraction notations\n",
    "    normalized = normalized.replace(\"dfrac\", \"frac\")\n",
    "    normalized = normalized.replace(\"tfrac\", \"frac\")\n",
    "\n",
    "    # Normalize spaces\n",
    "    normalized = re.sub(r\"\\s+\", \"\", normalized)\n",
    "\n",
    "    # Normalize percentages\n",
    "    normalized = normalized.replace(\"\\\\%\", \"\")\n",
    "\n",
    "    # Normalize funny commas\n",
    "    normalized = normalized.replace(\"{,}\", \"\")\n",
    "\n",
    "    # Normalize common mathematical notations\n",
    "    normalized = normalized.replace(\"\\\\times\", \"*\")\n",
    "    normalized = normalized.replace(\"\\\\cdot\", \"*\")\n",
    "\n",
    "    # Normalize decimal representation\n",
    "    normalized = re.sub(r\"(\\d+)[\\.,](\\d+)\", r\"\\1.\\2\", normalized)\n",
    "\n",
    "    # Remove unnecessary braces in simple expressions\n",
    "    normalized = re.sub(r\"{([^{}]+)}\", r\"\\1\", normalized)\n",
    "\n",
    "    # Normalize common constants\n",
    "    normalized = normalized.replace(\"\\\\pi\", \"pi\")\n",
    "\n",
    "    # Remove LaTeX text commands\n",
    "    normalized = re.sub(r\"\\\\text\\{([^{}]+)\\}\", r\"\\1\", normalized)\n",
    "    normalized = re.sub(r\"\\\\mathrm\\{([^{}]+)\\}\", r\"\\1\", normalized)\n",
    "\n",
    "    # Normalize date formats (e.g., \"October 30\" vs \"October\\\\ 30\")\n",
    "    normalized = re.sub(r\"([a-z]+)\\\\+\\s*(\\d+)\", r\"\\1\\2\", normalized)\n",
    "    normalized = normalized.replace(\"\\\\text\", \"\")\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04490bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_latex_for_sympy(latex_str):\n",
    "    \"\"\"\n",
    "    Prepare a LaTeX string for SymPy parsing by removing unsupported commands\n",
    "    and simplifying the expression.\n",
    "    \"\"\"\n",
    "    if not isinstance(latex_str, str):\n",
    "        return str(latex_str)\n",
    "\n",
    "    # Remove \\boxed{} command\n",
    "    latex_str = re.sub(r\"\\\\boxed\\{(.*?)\\}\", r\"\\1\", latex_str)\n",
    "\n",
    "    # Replace common LaTeX commands that SymPy doesn't support\n",
    "    replacements = {\n",
    "        r\"\\\\dfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\tfrac\": r\"\\\\frac\",\n",
    "        r\"\\\\cdot\": r\"*\",\n",
    "        r\"\\\\times\": r\"*\",\n",
    "        r\"\\\\div\": r\"/\",\n",
    "        r\"\\\\left\": r\"\",\n",
    "        r\"\\\\right\": r\"\",\n",
    "        r\"\\\\textbf\": r\"\",\n",
    "        r\"\\\\text\": r\"\",\n",
    "        r\"\\\\mathrm\": r\"\",\n",
    "        r\"\\\\!\": r\"\",\n",
    "        r\",\": r\"\",\n",
    "    }\n",
    "\n",
    "    for old, new in replacements.items():\n",
    "        latex_str = re.sub(old, new, latex_str)\n",
    "\n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d10d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(answer: str, gt_answer: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the generated answer matches the ground truth answer\n",
    "    after normalizing LaTeX formatting.\n",
    "\n",
    "    Args:\n",
    "        answer: The generated answer to check\n",
    "        gt_answer: The ground truth answer to compare against\n",
    "\n",
    "    Returns:\n",
    "        True if the answers match after normalization, False otherwise\n",
    "    \"\"\"\n",
    "    # Normalize both answers\n",
    "    normalized_answer = normalize_latex(answer)\n",
    "    normalized_gt_answer = normalize_latex(gt_answer)\n",
    "\n",
    "    # First check if normalized strings match\n",
    "    if normalized_answer == normalized_gt_answer:\n",
    "        return True\n",
    "\n",
    "    # # If string comparison fails, try mathematical equivalence\n",
    "    # try:\n",
    "    #     return get_latex_equivalent(answer, gt_answer)\n",
    "    # except Exception as e:\n",
    "    #     # If SymPy parsing fails, fall back to string comparison result\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85c36094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_solution_into_chunks(solution_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a solution into chunks for rollout generation.\n",
    "\n",
    "    Args:\n",
    "        solution_text: The full solution text\n",
    "\n",
    "    Returns:\n",
    "        List of chunks\n",
    "    \"\"\"\n",
    "    # First, remove the prompt part if present\n",
    "    if \"<think>\" in solution_text:\n",
    "        solution_text = solution_text.split(\"<think>\")[1].strip()\n",
    "\n",
    "    # Remove the closing tag if present\n",
    "    if \"</think>\" in solution_text:\n",
    "        solution_text = solution_text.split(\"</think>\")[0].strip()\n",
    "\n",
    "    # Define patterns for chunk boundaries\n",
    "    sentence_ending_tokens = [\".\", \"?\", \"!\"]\n",
    "    paragraph_ending_patterns = [\"\\n\\n\", \"\\r\\n\\r\\n\"]\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # Process the text character by character\n",
    "    i = 0\n",
    "    while i < len(solution_text):\n",
    "        current_chunk += solution_text[i]\n",
    "\n",
    "        # Check for paragraph endings\n",
    "        is_paragraph_end = False\n",
    "        for pattern in paragraph_ending_patterns:\n",
    "            if (\n",
    "                i + len(pattern) <= len(solution_text)\n",
    "                and solution_text[i : i + len(pattern)] == pattern\n",
    "            ):\n",
    "                is_paragraph_end = True\n",
    "                break\n",
    "\n",
    "        # Check for sentence endings followed by space or newline\n",
    "        is_sentence_end = False\n",
    "        if i < len(solution_text) - 1 and solution_text[i] in sentence_ending_tokens:\n",
    "            next_char = solution_text[i + 1]\n",
    "            if next_char == \" \" or next_char == \"\\n\":\n",
    "                is_sentence_end = True\n",
    "\n",
    "        # If we found a boundary, add the chunk and reset\n",
    "        if is_paragraph_end or is_sentence_end:\n",
    "            if current_chunk.strip():\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # # Add the last chunk if not empty\n",
    "    # if current_chunk.strip():\n",
    "    #     chunks.append(current_chunk.strip())\n",
    "    #     chunk_idxs.append(len(solution_text) - 1)  # Add last index\n",
    "\n",
    "    # Merge small chunks (less than 10 characters)\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        if len(chunks[i]) < 10:\n",
    "            # If this is the last chunk, merge with previous chunk if possible\n",
    "            if i == len(chunks) - 1:\n",
    "                if i > 0:\n",
    "                    chunks[i - 1] = chunks[i - 1] + \" \" + chunks[i]\n",
    "                    chunks.pop(i)\n",
    "            # Otherwise merge with the next chunk\n",
    "            else:\n",
    "                chunks[i + 1] = chunks[i] + \" \" + chunks[i + 1]\n",
    "                chunks.pop(i)\n",
    "                # Don't increment i since we need to check the new merged chunk\n",
    "            # If we're at the beginning and there's only one chunk, just keep it\n",
    "            if i == 0 and len(chunks) == 1:\n",
    "                break\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # chunk_boundaries = [(chunk_idxs[i], chunk_idxs[i + 1]) for i in range(len(chunk_idxs) - 1)]\n",
    "    # chunk_boundaries.append((chunk_idxs[-1], len(solution_text)))\n",
    "\n",
    "    # if get_idxs:\n",
    "    #     return chunks, chunk_boundaries\n",
    "    # else:\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90770fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grouped_answer_kl_divergence(rollout_answer_correct, cos_sims, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between answer correctness distributions for similar vs dissimilar rollouts.\n",
    "    This follows the thought-anchors approach.\n",
    "    \n",
    "    Args:\n",
    "        rollout_answer_correct: List of boolean correctness for each rollout\n",
    "        cos_sims: Tensor of cosine similarities for each rollout\n",
    "        similarity_threshold: Threshold for determining similar vs dissimilar\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains KL divergences and group statistics\n",
    "    \"\"\"\n",
    "    # Ensure cos_sims is a tensor\n",
    "    if not torch.is_tensor(cos_sims):\n",
    "        cos_sims = torch.tensor(cos_sims, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    # Separate rollouts into similar and dissimilar groups\n",
    "    similar_mask = cos_sims > similarity_threshold\n",
    "    dissimilar_mask = ~similar_mask\n",
    "    \n",
    "    similar_correctness = [rollout_answer_correct[i] for i in range(len(rollout_answer_correct)) if similar_mask[i]]\n",
    "    dissimilar_correctness = [rollout_answer_correct[i] for i in range(len(rollout_answer_correct)) if dissimilar_mask[i]]\n",
    "    \n",
    "    if len(similar_correctness) == 0 or len(dissimilar_correctness) == 0:\n",
    "        return {\n",
    "            \"kl_divergence\": 0.0,\n",
    "            \"similar_group_size\": len(similar_correctness),\n",
    "            \"dissimilar_group_size\": len(dissimilar_correctness),\n",
    "            \"similar_accuracy\": 0.0,\n",
    "            \"dissimilar_accuracy\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Calculate accuracy rates for each group\n",
    "    similar_accuracy = sum(similar_correctness) / len(similar_correctness)\n",
    "    dissimilar_accuracy = sum(dissimilar_correctness) / len(dissimilar_correctness)\n",
    "    \n",
    "    # Create probability distributions\n",
    "    # Similar distribution: [P(wrong), P(correct)]\n",
    "    similar_dist = torch.tensor([1 - similar_accuracy, similar_accuracy], dtype=torch.float32)\n",
    "    \n",
    "    # Dissimilar distribution: [P(wrong), P(correct)]\n",
    "    dissimilar_dist = torch.tensor([1 - dissimilar_accuracy, dissimilar_accuracy], dtype=torch.float32)\n",
    "    \n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-8\n",
    "    similar_dist = similar_dist + eps\n",
    "    dissimilar_dist = dissimilar_dist + eps\n",
    "    \n",
    "    # Normalize to ensure they sum to 1\n",
    "    similar_dist = similar_dist / similar_dist.sum()\n",
    "    dissimilar_dist = dissimilar_dist / dissimilar_dist.sum()\n",
    "    \n",
    "    # Calculate KL divergence: KL(P_dissimilar || P_similar)\n",
    "    # This measures how much the dissimilar group diverges from similar group\n",
    "    kl_div = torch.sum(dissimilar_dist * torch.log(dissimilar_dist / similar_dist))\n",
    "    \n",
    "    return {\n",
    "        \"kl_divergence\": float(kl_div),\n",
    "        \"similar_group_size\": len(similar_correctness),\n",
    "        \"dissimilar_group_size\": len(dissimilar_correctness),\n",
    "        \"similar_accuracy\": similar_accuracy,\n",
    "        \"dissimilar_accuracy\": dissimilar_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f43a96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_max_tokens_for_sentence(sentence_idx):\n",
    "#     \"\"\"Dynamic token allocation based on sentence position.\"\"\"\n",
    "#     if sentence_idx < 20:\n",
    "#         return 6000  # Full CoT for very early sentences\n",
    "#     elif sentence_idx < 40:\n",
    "#         return 5000  # Slightly reduced\n",
    "#     elif sentence_idx < :\n",
    "#         return 4500  # Moderate reduction\n",
    "#     else:\n",
    "#         return 4000  # Conservative for very late sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a68f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diverse_rollouts(model, tokenizer, ground_truth_answer, context, num_rollouts=10, batch_size=5, temperature=0.6, top_p=0.95, sentence_idx=0):\n",
    "    \"\"\"Generate diverse text completions in batches for better GPU utilization.\"\"\"\n",
    "\n",
    "    # START: Clean memory before beginning (NEW)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", max_length=1500, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Dynamic token allocation based on sentence position\n",
    "    max_new_tokens = 6000\n",
    "\n",
    "    # Add </think> as a stop sequence\n",
    "    stop_token_ids = tokenizer.encode(\"</think>\", add_special_tokens=False)\n",
    "    rollout_texts = []\n",
    "    rollout_answer_correct = [] # this uses contains_answer\n",
    "    rollout_answer_correct_check = []  # New list using check_answer\n",
    "    \n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in range(0, num_rollouts, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_rollouts)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        \n",
    "        # Expand inputs for batch processing\n",
    "        batch_inputs = {\n",
    "            'input_ids': inputs['input_ids'].repeat(current_batch_size, 1),\n",
    "            'attention_mask': inputs['attention_mask'].repeat(current_batch_size, 1)\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                batch_inputs['input_ids'],\n",
    "                attention_mask=batch_inputs['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                output_hidden_states=True,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "\n",
    "        # Process each sequence in the batch\n",
    "        batch_generated_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        batch_generated_texts = tokenizer.batch_decode(batch_generated_ids, skip_special_tokens=True)\n",
    "        # 2. BATCH STRIP ALL TEXTS\n",
    "        batch_texts_stripped = [text.strip() for text in batch_generated_texts]\n",
    "        rollout_texts.extend(batch_texts_stripped)\n",
    "        \n",
    "        # 3. BATCH CHECK ANSWER CORRECTNESS (contains_answer)\n",
    "        batch_contains_answer = [contains_answer(text, ground_truth_answer) for text in batch_texts_stripped]\n",
    "        rollout_answer_correct.extend(batch_contains_answer)\n",
    "        \n",
    "        # 4. BATCH EXTRACT AND CHECK BOXED ANSWERS\n",
    "        batch_boxed_answers = [extract_boxed_answers(text) for text in batch_texts_stripped]\n",
    "        batch_correct_check = [\n",
    "            any(check_answer(answer, ground_truth_answer) for answer in boxed_answers)\n",
    "            for boxed_answers in batch_boxed_answers\n",
    "        ]\n",
    "        rollout_answer_correct_check.extend(batch_correct_check)\n",
    "                \n",
    "        # ADD THIS: Clean up after each batch\n",
    "        del outputs, batch_inputs, batch_generated_ids, batch_generated_texts\n",
    "        del batch_texts_stripped, batch_contains_answer, batch_boxed_answers, batch_correct_check\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    # Final cleanup\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Stack embeddings into a single tensor on GPU\n",
    "    return rollout_texts, rollout_answer_correct, rollout_answer_correct_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90c79f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_embedding():\n",
    "#     rollout_llm_embeddings = []\n",
    "#     rollout_sentence_embeddings = []\n",
    "#     rollout_sentences = []\n",
    "#     for rollout_text in rollout_texts:\n",
    "#         rollout_resampled = split_solution_into_chunks(rollout_text)[0]\n",
    "#         llm_embedding, sent_embedding = get_both_embeddings(model, sentence_model, tokenizer, rollout_resampled)\n",
    "#         rollout_llm_embeddings.append(llm_embedding)\n",
    "#         rollout_sentence_embeddings.append(sent_embedding)\n",
    "#         rollout_sentences.append(rollout_resampled)\n",
    "#     # Stack embeddings\n",
    "#     rollout_llm_embeddings = torch.stack(rollout_llm_embeddings)\n",
    "#     rollout_sentence_embeddings = torch.stack(rollout_sentence_embeddings)\n",
    "    \n",
    "#     # Calculate cosine similarities using PyTorch batch operations\n",
    "#     cos_sims_llm = torch.cosine_similarity(\n",
    "#         original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "#     )\n",
    "#     cos_sims_sent = torch.cosine_similarity(\n",
    "#         original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "#     )\n",
    "\n",
    "#     print(f\"cos_sims_llm{sentence_idx}: {cos_sims_llm}\")\n",
    "#     print(f\"cos_sims_sent{sentence_idx}: {cos_sims_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63a609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embeddings_batch(llm_model, sentence_model, tokenizer, texts):\n",
    "    \"\"\"Get both LLM and sentence transformer embeddings for multiple texts in batches.\"\"\"\n",
    "    \n",
    "    # Batch LLM embeddings\n",
    "    llm_inputs = tokenizer(texts, return_tensors=\"pt\", max_length=2048, truncation=True, padding=True)\n",
    "    device = next(llm_model.parameters()).device\n",
    "    llm_inputs = {k: v.to(device) for k, v in llm_inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model(**llm_inputs, output_hidden_states=True)\n",
    "        # Get last token embeddings for each sequence\n",
    "        llm_embeddings = outputs.hidden_states[-1][:, -1, :].float().cpu()\n",
    "    \n",
    "    # Batch sentence transformer embeddings\n",
    "    sent_embeddings = sentence_model.encode(texts, convert_to_tensor=True, batch_size=32).float().cpu()\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del outputs, llm_inputs\n",
    "    \n",
    "    return llm_embeddings, sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64cc9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_sentence(model, tokenizer, problem_text, allsentences, ground_truth_answer,  num_rollouts=20, sentence_idx=0, batch=5):\n",
    "    #sentence_idx sentence position\n",
    "    \"\"\"Save raw rollout data and similarities for later importance calculation.\"\"\"\n",
    "    \n",
    "    # START: Clean memory before beginning\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Context WITHOUT the current sentence\n",
    "    print(f\"Context removed: {allsentences[sentence_idx]}\")\n",
    "    prefix_text = allsentences[:sentence_idx]\n",
    "    context_without = problem_text + \" \" + \" \".join(prefix_text) + \"\"\n",
    "\n",
    "    # Generate diverse rollouts from context without chunk\n",
    "    rollout_texts, rollout_answer_correct, rollout_answer_correct_check = generate_diverse_rollouts(\n",
    "        model,                    \n",
    "        tokenizer,               \n",
    "        ground_truth_answer,     \n",
    "        context_without,         \n",
    "        num_rollouts=num_rollouts,    \n",
    "        batch_size=batch,             \n",
    "        temperature=0.6,              \n",
    "        top_p=0.95,\n",
    "        sentence_idx=sentence_idx                     \n",
    "    )\n",
    "    \n",
    "    # Get original sentence embeddings\n",
    "    original_text = allsentences[sentence_idx]\n",
    "    original_llm_emb, original_sent_emb = get_both_embeddings(model, sentence_model, tokenizer, original_text)\n",
    "\n",
    "    # BATCH PROCESS ROLLOUTS\n",
    "    rollout_sentences = [split_solution_into_chunks(rollout_text)[0] for rollout_text in rollout_texts]\n",
    "    \n",
    "    # Get all embeddings at once\n",
    "    rollout_llm_embeddings, rollout_sentence_embeddings = get_both_embeddings_batch(\n",
    "        model, sentence_model, tokenizer, rollout_sentences\n",
    "    )\n",
    "    \n",
    "    # Calculate cosine similarities using PyTorch batch operations\n",
    "    cos_sims_llm = torch.cosine_similarity(\n",
    "        original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "    )\n",
    "    cos_sims_sent = torch.cosine_similarity(\n",
    "        original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "    )\n",
    "\n",
    "    print(f\"cos_sims_llm{sentence_idx}: {cos_sims_llm}\")\n",
    "    print(f\"cos_sims_sent{sentence_idx}: {cos_sims_sent}\")\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    unique_responses = len(set(rollout_texts))\n",
    "    \n",
    "    # Store raw data for later importance calculation\n",
    "    result = {\n",
    "        \"problem_id\": None,  # Will be set by caller\n",
    "        \"sentence_idx\": sentence_idx,\n",
    "        \"sentence_text\": original_text,\n",
    "        \"function_tags\": [],  # Will be set by caller\n",
    "        \n",
    "        # Context information\n",
    "        \"context_without_sentence\": context_without,\n",
    "        \"ground_truth_answer\": ground_truth_answer,\n",
    "        \n",
    "        # Rollout data\n",
    "        \"num_rollouts\": num_rollouts,\n",
    "        \"rollout_sentences\": rollout_sentences,\n",
    "        \"rollout_answer_correct\": rollout_answer_correct,\n",
    "        \"rollout_answer_correct_check\": rollout_answer_correct_check,\n",
    "        \"unique_responses\": unique_responses,\n",
    "\n",
    "        \n",
    "        # extracted boxed_answers\n",
    "        \"rollout_boxed_answers\": [extract_boxed_answers(text) for text in rollout_texts],\n",
    "        # Raw similarity scores (convert to lists for JSON serialization)\n",
    "        \"cos_sims_llm\": cos_sims_llm.cpu().tolist(),\n",
    "        \"cos_sims_sentence\": cos_sims_sent.cpu().tolist(),\n",
    "        \n",
    "        # Summary statistics for quick reference\n",
    "        # \"llm_similarity_stats\": {\n",
    "        #     \"mean\": float(torch.mean(cos_sims_llm)),\n",
    "        #     \"std\": float(torch.std(cos_sims_llm)),\n",
    "        #     \"min\": float(torch.min(cos_sims_llm)),\n",
    "        #     \"max\": float(torch.max(cos_sims_llm))\n",
    "        # },\n",
    "        # \"sentence_similarity_stats\": {\n",
    "        #     \"mean\": float(torch.mean(cos_sims_sent)),\n",
    "        #     \"std\": float(torch.std(cos_sims_sent)),\n",
    "        #     \"min\": float(torch.min(cos_sims_sent)),\n",
    "        #     \"max\": float(torch.max(cos_sims_sent))\n",
    "        # },\n",
    "        \n",
    "        # Embedding comparison\n",
    "        \n",
    "        # Store original embeddings for potential later use (optional)\n",
    "        # \"original_llm_embedding\": original_llm_emb.cpu().tolist(),\n",
    "        # \"original_sent_embedding\": original_sent_emb.cpu().tolist(),\n",
    "        \n",
    "        # Metadata for importance calculation\n",
    "        \"generation_params\": {\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95,\n",
    "            \"batch_size\": batch\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    del rollout_texts, rollout_llm_embeddings, rollout_sentence_embeddings, rollout_answer_correct, rollout_answer_correct_check\n",
    "    del cos_sims_llm, cos_sims_sent, rollout_sentences\n",
    "    del original_llm_emb, original_sent_emb\n",
    "    if 'original_text' in locals():\n",
    "        del original_text\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a627ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_tags = [\n",
    "#         'uncertainty_management', \n",
    "#         'plan_generation'\n",
    "#     ]\n",
    "# def extract_target_sentence_indices(all_problem_labels, target_tags):\n",
    "#     \"\"\"\n",
    "#     Extract sentence indices that have the target function tags.\n",
    "    \n",
    "#     Returns a dictionary mapping problem_id to list of sentence indices to process.\n",
    "#     \"\"\"\n",
    "#     target_indices = {}\n",
    "    \n",
    "#     for problem in all_problem_labels:\n",
    "#         problem_id = problem['problem_id']\n",
    "#         indices_to_process = []\n",
    "        \n",
    "#         for i, chunk_data in enumerate(problem['chunks']):\n",
    "#             function_tags = chunk_data.get('function_tags', [])\n",
    "            \n",
    "#             # Check if any target tags are in this chunk's function_tags\n",
    "#             if any(tag in function_tags for tag in target_tags):\n",
    "#                 indices_to_process.append(i)\n",
    "        \n",
    "#         if indices_to_process:  # Only add if we found relevant chunks\n",
    "#             target_indices[problem_id] = indices_to_process\n",
    "    \n",
    "#     return target_indices\n",
    "\n",
    "# # Extract target sentence indices\n",
    "# target_sentence_indices = extract_target_sentence_indices(all_problem_labels, target_tags)\n",
    "# target_sentence_indices = extract_target_sentence_indices(all_problem_labels, target_tags)\n",
    "\n",
    "# print(f\"Found target sentences in {len(target_sentence_indices)} problems\")\n",
    "\n",
    "# for prompt, label in zip(all_prompt[:1], all_problem_labels[:1]):\n",
    "#     problem_id = prompt[\"problem_id\"]\n",
    "#     problem_text_prompt = prompt[\"problem_statement\"]\n",
    "#     allsentences = prompt[\"sentences\"]\n",
    "#     ground_truth_answer = prompt[\"answer\"]\n",
    "#     problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "#     # Get target sentence indices for this problem\n",
    "#     if problem_id not in target_sentence_indices:\n",
    "#         print(f\"\\nSkipping problem {problem_id} - no target function tags found\")\n",
    "#         continue\n",
    "    \n",
    "#     target_indices = target_sentence_indices[problem_id]\n",
    "#     print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} total sentences\")\n",
    "#     print(f\"Target sentence indices to process: {target_indices}\")\n",
    "    \n",
    "#     # Process only target sentences\n",
    "#     sentence_results = []\n",
    "#     for sentence_idx in target_indices:\n",
    "#         if sentence_idx >= len(allsentences):\n",
    "#             print(f\"Warning: sentence_idx {sentence_idx} >= len(allsentences) {len(allsentences)}\")\n",
    "#             continue\n",
    "            \n",
    "#         print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} (target) ---\")\n",
    "#         print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "#         print(f\"Function tags: {label['chunks'][sentence_idx].get('function_tags', [])}\")\n",
    "        \n",
    "#         sentence_result = process_one_sentence(\n",
    "#             model, tokenizer, problem_text, allsentences, ground_truth_answer, \n",
    "#             num_rollouts=num_rollouts, sentence_idx=sentence_idx, batch=batch\n",
    "#         )\n",
    "        \n",
    "#         # Add problem metadata and function tags\n",
    "#         sentence_result[\"problem_id\"] = problem_id\n",
    "#         sentence_result[\"sentence_idx\"] = sentence_idx\n",
    "#         sentence_result[\"sentence_text\"] = allsentences[sentence_idx]\n",
    "#         sentence_result[\"function_tags\"] = label['chunks'][sentence_idx].get('function_tags', [])\n",
    "#         sentence_results.append(sentence_result)\n",
    "        \n",
    "#         # Cleanup after each sentence\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "#     results.extend(sentence_results)\n",
    "#     print(f\"\\nCompleted problem {problem_id} - processed {len(sentence_results)} target sentences\")\n",
    "\n",
    "# print(f\"\\nProcessed {len(results)} target sentences across all problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c12118ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_problem(\n",
    "    problem_data: Dict, \n",
    "    problem_labels: Dict,\n",
    "    model, \n",
    "    tokenizer, \n",
    "    sentence_model,\n",
    "    output_dir: str = \"rollout_results\",\n",
    "    num_rollouts: int = 20,\n",
    "    batch_size: int = 5,\n",
    "    force: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single problem: loop through every sentence and generate rollouts.\n",
    "    Can resume from where it left off if results already exist.\n",
    "    \"\"\"\n",
    "    problem_id = problem_data[\"problem_id\"]\n",
    "    problem_text_prompt = problem_data[\"problem_statement\"]\n",
    "    allsentences = problem_data[\"sentences\"]\n",
    "    ground_truth_answer = problem_data[\"answer\"]\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_path = Path(output_dir)\n",
    "    problem_dir = output_path / problem_id\n",
    "    problem_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Define the results file path\n",
    "    results_file = problem_dir / \"sentence_rollouts.json\"\n",
    "    \n",
    "    # Check if we can resume from existing results\n",
    "    sentence_results = []\n",
    "    start_sentence_idx = 0\n",
    "    \n",
    "    if results_file.exists() and not force:\n",
    "        try:\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                existing_results = json.load(f)\n",
    "            \n",
    "            # Filter out failed results to find the last successful sentence\n",
    "            successful_results = [r for r in existing_results if \"error\" not in r]\n",
    "            \n",
    "            if successful_results:\n",
    "                sentence_results = existing_results\n",
    "                start_sentence_idx = len(successful_results)\n",
    "                print(f\"Resuming from sentence {start_sentence_idx + 1}/{len(allsentences)}\")\n",
    "                print(f\"Found {len(successful_results)} existing successful results\")\n",
    "            else:\n",
    "                print(f\"Found existing file but no successful results. Starting from beginning.\")\n",
    "                \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error reading existing results file: {e}. Starting from beginning.\")\n",
    "    \n",
    "    # Prepare problem text for generation\n",
    "    problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "    print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} sentences\")\n",
    "    \n",
    "    # Process sentences starting from where we left off\n",
    "    for sentence_idx in range(start_sentence_idx, len(allsentences)):\n",
    "        print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} ---\")\n",
    "        print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "        \n",
    "        # Get function tags if available\n",
    "        function_tags = []\n",
    "        if sentence_idx < len(problem_labels.get('chunks', [])):\n",
    "            function_tags = problem_labels['chunks'][sentence_idx].get('function_tags', [])\n",
    "            print(f\"Function tags: {function_tags}\")\n",
    "        \n",
    "        try:\n",
    "            # Process this sentence\n",
    "            sentence_result = process_one_sentence(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                problem_text=problem_text,\n",
    "                allsentences=allsentences,\n",
    "                ground_truth_answer=ground_truth_answer,\n",
    "                num_rollouts=num_rollouts,\n",
    "                sentence_idx=sentence_idx,\n",
    "                batch=batch_size\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            sentence_result[\"problem_id\"] = problem_id\n",
    "            sentence_result[\"sentence_idx\"] = sentence_idx\n",
    "            sentence_result[\"sentence_text\"] = allsentences[sentence_idx]\n",
    "            sentence_result[\"function_tags\"] = function_tags\n",
    "            \n",
    "            sentence_results.append(sentence_result)\n",
    "            \n",
    "            print(f\"Sentence {sentence_idx + 1}: Completed successfully\")\n",
    "            print(f\"  - Unique responses: {sentence_result['unique_responses']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence {sentence_idx}: {e}\")\n",
    "            # Still save partial results\n",
    "            error_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": allsentences[sentence_idx],\n",
    "                \"function_tags\": function_tags,\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "            sentence_results.append(error_result)\n",
    "        \n",
    "        # Cleanup after each sentence\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save intermediate results every 5 sentences or immediately if we're resuming\n",
    "        if (sentence_idx + 1) % 5 == 0 or sentence_idx == start_sentence_idx:\n",
    "            with open(results_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(sentence_results, f, indent=2, default=str)\n",
    "            print(f\"Saved intermediate results after sentence {sentence_idx + 1}\")\n",
    "    \n",
    "    # Save final results\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sentence_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary = {\n",
    "        \"problem_id\": problem_id,\n",
    "        \"total_sentences\": len(allsentences),\n",
    "        \"processed_sentences\": len(sentence_results),\n",
    "        \"successful_sentences\": len([r for r in sentence_results if \"error\" not in r]),\n",
    "        \"failed_sentences\": len([r for r in sentence_results if \"error\" in r]),\n",
    "        \"generation_params\": {\n",
    "            \"num_rollouts\": num_rollouts,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95\n",
    "        },\n",
    "        \"resumed_from_sentence\": start_sentence_idx\n",
    "    }\n",
    "    \n",
    "    summary_file = problem_dir / \"processing_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for temp_file in problem_dir.glob(\"sentence_rollouts_temp_*.json\"):\n",
    "        temp_file.unlink()\n",
    "    \n",
    "    print(f\"\\nCompleted problem {problem_id}\")\n",
    "    print(f\"  - Total sentences: {len(allsentences)}\")\n",
    "    print(f\"  - Successfully processed: {summary['successful_sentences']}\")\n",
    "    print(f\"  - Failed: {summary['failed_sentences']}\")\n",
    "    print(f\"  - Results saved to: {results_file}\")\n",
    "    \n",
    "    return sentence_results\n",
    "\n",
    "def process_multiple_problems(\n",
    "    all_prompt: List[Dict],\n",
    "    all_problem_labels: List[Dict],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sentence_model,\n",
    "    output_dir: str = \"rollout_results\",\n",
    "    num_rollouts: int = 20,\n",
    "    batch_size: int = 5,\n",
    "    force: bool = False,\n",
    "    max_problems: int = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process multiple problems sequentially.\n",
    "    Can resume from partially completed problems.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Limit number of problems if specified\n",
    "    problems_to_process = all_prompt[:max_problems] if max_problems else all_prompt\n",
    "    labels_to_process = all_problem_labels[:max_problems] if max_problems else all_problem_labels\n",
    "    \n",
    "    print(f\"Processing {len(problems_to_process)} problems\")\n",
    "    \n",
    "    all_sentence_results = []\n",
    "    \n",
    "    for i, (problem_data, problem_labels) in enumerate(zip(problems_to_process, labels_to_process)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing problem {i+1}/{len(problems_to_process)}: {problem_data['problem_id']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            sentence_results = process_single_problem(\n",
    "                problem_data=problem_data,\n",
    "                problem_labels=problem_labels,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                sentence_model=sentence_model,\n",
    "                output_dir=output_dir,\n",
    "                num_rollouts=num_rollouts,\n",
    "                batch_size=batch_size,\n",
    "                force=force\n",
    "            )\n",
    "            all_sentence_results.extend(sentence_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process problem {problem_data['problem_id']}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Final cleanup between problems\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nCompleted processing all problems. Results saved in: {output_dir}\")\n",
    "    print(f\"Total sentence results: {len(all_sentence_results)}\")\n",
    "    \n",
    "    return all_sentence_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aeffa338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_problem_with_multi_head_ablation(\n",
    "    problem_data: Dict, \n",
    "    problem_labels: Dict,\n",
    "    model, \n",
    "    tokenizer, \n",
    "    sentence_model,\n",
    "    receiver_heads: List[tuple],  # List of (layer_idx, head_idx) tuples\n",
    "    output_dir: str = \"rollout_results_ablation\",\n",
    "    num_rollouts: int = 6,\n",
    "    batch_size: int = 6,\n",
    "    force: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single problem with receiver head ablation: loop through every sentence \n",
    "    and ablate ALL receiver heads simultaneously, storing rollouts and similarities.\n",
    "    \"\"\"\n",
    "    problem_id = problem_data[\"problem_id\"]\n",
    "    problem_text_prompt = problem_data[\"problem_statement\"]\n",
    "    allsentences = problem_data[\"sentences\"]\n",
    "    ground_truth_answer = problem_data[\"answer\"]\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_path = Path(output_dir)\n",
    "    problem_dir = output_path / problem_id\n",
    "    problem_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Prepare problem text for generation\n",
    "    problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "    print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} sentences\")\n",
    "    print(f\"Will ablate {len(receiver_heads)} receiver heads SIMULTANEOUSLY per sentence\")\n",
    "    \n",
    "    all_ablation_results = []\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence_idx in range(len(allsentences)):\n",
    "        print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} ---\")\n",
    "        print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "        \n",
    "        # Get context without current sentence (same as baseline)\n",
    "        prefix_text = allsentences[:sentence_idx]\n",
    "        context_without = problem_text + \" \" + \" \".join(prefix_text) + \"\"\n",
    "        \n",
    "        # Get function tags if available\n",
    "        function_tags = []\n",
    "        if sentence_idx < len(problem_labels.get('chunks', [])):\n",
    "            function_tags = problem_labels['chunks'][sentence_idx].get('function_tags', [])\n",
    "        \n",
    "        # Get original sentence embeddings\n",
    "        original_text = allsentences[sentence_idx]\n",
    "        original_llm_emb, original_sent_emb = get_both_embeddings(model, sentence_model, tokenizer, original_text)\n",
    "        \n",
    "        # CREATE ABLATION HOOKS FOR ALL HEADS\n",
    "        hooks = []\n",
    "        print(f\"  Adding hooks for {len(receiver_heads)} heads...\")\n",
    "        \n",
    "        for layer_idx, head_num in receiver_heads:\n",
    "            print(f\"    Adding hook for head ({layer_idx}, {head_num})\")\n",
    "            \n",
    "            # Define ablation hook for this specific head\n",
    "            def create_ablation_hook(target_head_num):\n",
    "                def ablation_hook(module, input, output):\n",
    "                    attention_output = output[0]\n",
    "                    batch_size_tensor, seq_len, hidden_dim = attention_output.shape\n",
    "                    num_heads = module.num_attention_heads\n",
    "                    head_dim = hidden_dim // num_heads\n",
    "                    \n",
    "                    # Zero out the specific head\n",
    "                    reshaped = attention_output.view(batch_size_tensor, seq_len, num_heads, head_dim)\n",
    "                    reshaped[:, :, target_head_num, :] = 0\n",
    "                    modified = reshaped.view(batch_size_tensor, seq_len, hidden_dim)\n",
    "                    \n",
    "                    return (modified,) + output[1:]\n",
    "                return ablation_hook\n",
    "            \n",
    "            # Register hook for this layer\n",
    "            attention_layer = model.model.layers[layer_idx].self_attn\n",
    "            hook = attention_layer.register_forward_hook(create_ablation_hook(head_num))\n",
    "            hooks.append((hook, layer_idx, head_num))\n",
    "        \n",
    "        print(f\"  Successfully added {len(hooks)} hooks. Running generation...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate rollouts with ALL heads ablated simultaneously\n",
    "            rollout_texts, rollout_answer_correct, rollout_answer_correct_check = generate_diverse_rollouts(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer, \n",
    "                ground_truth_answer=ground_truth_answer,\n",
    "                context=context_without,\n",
    "                num_rollouts=num_rollouts,\n",
    "                batch_size=batch_size,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                sentence_idx=sentence_idx\n",
    "            )\n",
    "            \n",
    "            # Process rollout embeddings (same as baseline)\n",
    "            rollout_sentences = [split_solution_into_chunks(rollout_text)[0] for rollout_text in rollout_texts]\n",
    "            \n",
    "            # Get all embeddings at once\n",
    "            rollout_llm_embeddings, rollout_sentence_embeddings = get_both_embeddings_batch(\n",
    "                model, sentence_model, tokenizer, rollout_sentences\n",
    "            )\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            cos_sims_llm = torch.cosine_similarity(\n",
    "                original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "            )\n",
    "            cos_sims_sent = torch.cosine_similarity(\n",
    "                original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "            )\n",
    "            \n",
    "            # Calculate basic statistics\n",
    "            unique_responses = len(set(rollout_texts))\n",
    "            \n",
    "            # Store ablation result for ALL heads ablated together\n",
    "            ablation_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"function_tags\": function_tags,\n",
    "                \n",
    "                # Multi-head ablation info\n",
    "                \"ablated_heads\": receiver_heads,  # List of all ablated heads\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"num_ablated_heads\": len(receiver_heads),\n",
    "                \n",
    "                # Context information\n",
    "                \"context_without_sentence\": context_without,\n",
    "                \"ground_truth_answer\": ground_truth_answer,\n",
    "                \n",
    "                # Rollout data - SAME AS YOUR BASELINE\n",
    "                \"num_rollouts\": num_rollouts,\n",
    "                \"rollout_texts\": rollout_texts,  # Full rollout texts\n",
    "                \"rollout_sentences\": rollout_sentences,\n",
    "                \"rollout_answer_correct\": rollout_answer_correct,\n",
    "                \"rollout_answer_correct_check\": rollout_answer_correct_check,\n",
    "                \"unique_responses\": unique_responses,\n",
    "                \n",
    "                # Extracted boxed answers\n",
    "                \"rollout_boxed_answers\": [extract_boxed_answers(text) for text in rollout_texts],\n",
    "                \n",
    "                # Raw similarity scores\n",
    "                \"cos_sims_llm\": cos_sims_llm.cpu().tolist(),\n",
    "                \"cos_sims_sentence\": cos_sims_sent.cpu().tolist(),\n",
    "                \n",
    "                # Embedding comparison\n",
    "                \"embedding_correlation\": float(torch.corrcoef(torch.stack([cos_sims_llm, cos_sims_sent]))[0, 1]) if len(cos_sims_llm) > 1 else 0.0,\n",
    "                \n",
    "                # Metadata\n",
    "                \"generation_params\": {\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"batch_size\": batch_size\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            all_ablation_results.append(ablation_result)\n",
    "            \n",
    "            print(f\"  Successfully generated {len(rollout_texts)} rollouts with all heads ablated\")\n",
    "            print(f\"  Unique responses: {unique_responses}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del rollout_llm_embeddings, rollout_sentence_embeddings\n",
    "            del cos_sims_llm, cos_sims_sent, rollout_sentences\n",
    "            # rollout_texts will be garbage collected after saving\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error during multi-head ablation: {e}\")\n",
    "            # Store error result\n",
    "            error_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"ablated_heads\": receiver_heads,\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "            all_ablation_results.append(error_result)\n",
    "            \n",
    "        finally:\n",
    "            # ALWAYS remove all hooks\n",
    "            print(f\"  Removing {len(hooks)} hooks...\")\n",
    "            for hook, layer_idx, head_num in hooks:\n",
    "                hook.remove()\n",
    "            hooks.clear()\n",
    "        \n",
    "        # Cleanup after processing this sentence\n",
    "        del original_llm_emb, original_sent_emb\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save all results\n",
    "    results_file = problem_dir / \"sentence_multi_head_ablation_rollouts.json\"\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_ablation_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        \"problem_id\": problem_id,\n",
    "        \"total_sentences\": len(allsentences),\n",
    "        \"ablated_heads\": receiver_heads,\n",
    "        \"num_ablated_heads\": len(receiver_heads),\n",
    "        \"ablation_type\": \"multi_head_simultaneous\",\n",
    "        \"total_ablation_experiments\": len(allsentences),  # One experiment per sentence\n",
    "        \"successful_experiments\": len([r for r in all_ablation_results if \"error\" not in r]),\n",
    "        \"failed_experiments\": len([r for r in all_ablation_results if \"error\" in r]),\n",
    "        \"generation_params\": {\n",
    "            \"num_rollouts\": num_rollouts,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = problem_dir / \"multi_head_ablation_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nCompleted problem {problem_id}\")\n",
    "    print(f\"  - Total sentences: {len(allsentences)}\")\n",
    "    print(f\"  - Heads ablated simultaneously: {len(receiver_heads)}\")\n",
    "    print(f\"  - Total experiments: {len(allsentences)}\")\n",
    "    print(f\"  - Successful: {summary['successful_experiments']}\")\n",
    "    print(f\"  - Failed: {summary['failed_experiments']}\")\n",
    "    print(f\"  - Results saved to: {results_file}\")\n",
    "    \n",
    "    return all_ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c79f86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_problem_with_multi_head_ablation(\n",
    "    problem_data: Dict, \n",
    "    problem_labels: Dict,\n",
    "    model, \n",
    "    tokenizer, \n",
    "    sentence_model,\n",
    "    receiver_heads: List[tuple],  # List of (layer_idx, head_idx) tuples\n",
    "    output_dir: str = \"rollout_results_ablation\",\n",
    "    num_rollouts: int = 6,\n",
    "    batch_size: int = 6,\n",
    "    force: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single problem with receiver head ablation: loop through every sentence \n",
    "    and ablate ALL receiver heads simultaneously, storing rollouts and similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    problem_id = problem_data[\"problem_id\"]\n",
    "    problem_text_prompt = problem_data[\"problem_statement\"]\n",
    "    allsentences = problem_data[\"sentences\"]\n",
    "    ground_truth_answer = problem_data[\"answer\"]\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_path = Path(output_dir)\n",
    "    problem_dir = output_path / problem_id\n",
    "    problem_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Save all results\n",
    "    results_file = problem_dir / \"sentence_multi_head_ablation_rollouts.json\"\n",
    "    \n",
    "    # Prepare problem text for generation\n",
    "    problem_text = f\"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{{}}. Problem: {problem_text_prompt} Solution: \\n<think>\\n\"\n",
    "    \n",
    "    print(f\"\\nProcessing problem {problem_id} with {len(allsentences)} sentences\")\n",
    "    print(f\"Will ablate {len(receiver_heads)} receiver heads SIMULTANEOUSLY per sentence\")\n",
    "\n",
    "    # Check if we can resume from existing results\n",
    "    all_ablation_results = []\n",
    "    start_sentence_idx = 186 # start at last sentence processed\n",
    "    \n",
    "    if results_file.exists() and not force:\n",
    "        try:\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                existing_results = json.load(f)\n",
    "            \n",
    "            # Filter out failed results to find the last successful sentence\n",
    "            successful_results = [r for r in existing_results if \"error\" not in r]\n",
    "            \n",
    "            if successful_results:\n",
    "                all_ablation_results = existing_results\n",
    "                start_sentence_idx = len(successful_results)\n",
    "                print(f\"Resuming from sentence {start_sentence_idx + 1}/{len(allsentences)}\")\n",
    "                print(f\"Found {len(successful_results)} existing successful results\")\n",
    "            else:\n",
    "                print(f\"Found existing file but no successful results. Starting from beginning.\")\n",
    "                \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error reading existing results file: {e}. Starting from beginning.\")\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence_idx in range(start_sentence_idx, len(allsentences)):\n",
    "        print(f\"\\n--- Processing sentence {sentence_idx + 1}/{len(allsentences)} ---\")\n",
    "        print(f\"Sentence: {allsentences[sentence_idx]}\")\n",
    "        \n",
    "        # Get context without current sentence (same as baseline)\n",
    "        prefix_text = allsentences[:sentence_idx]\n",
    "        context_without = problem_text + \" \" + \" \".join(prefix_text) + \"\"\n",
    "        \n",
    "        # Get function tags if available\n",
    "        function_tags = []\n",
    "        if sentence_idx < len(problem_labels.get('chunks', [])):\n",
    "            function_tags = problem_labels['chunks'][sentence_idx].get('function_tags', [])\n",
    "        \n",
    "        # Get original sentence embeddings\n",
    "        original_text = allsentences[sentence_idx]\n",
    "        original_llm_emb, original_sent_emb = get_both_embeddings(model, sentence_model, tokenizer, original_text)\n",
    "        \n",
    "        # CREATE ABLATION HOOKS FOR ALL HEADS\n",
    "        hooks = []\n",
    "        print(f\"  Adding hooks for {len(receiver_heads)} heads...\")\n",
    "        \n",
    "        for layer_idx, head_num in receiver_heads:\n",
    "            print(f\"    Adding hook for head ({layer_idx}, {head_num})\")\n",
    "            \n",
    "            # Define ablation hook for this specific head\n",
    "            def create_ablation_hook(target_head_num):\n",
    "                def ablation_hook(module, input, output):\n",
    "                    attention_output = output[0]\n",
    "                    batch_size_tensor, seq_len, hidden_dim = attention_output.shape\n",
    "                    num_heads = getattr(module, 'num_heads', getattr(module, 'num_attention_heads', 32))\n",
    "                    head_dim = hidden_dim // num_heads\n",
    "                    \n",
    "                    # Zero out the specific head\n",
    "                    reshaped = attention_output.view(batch_size_tensor, seq_len, num_heads, head_dim)\n",
    "                    reshaped[:, :, target_head_num, :] = 0\n",
    "                    modified = reshaped.view(batch_size_tensor, seq_len, hidden_dim)\n",
    "                    \n",
    "                    return (modified,) + output[1:]\n",
    "                return ablation_hook\n",
    "            \n",
    "            # Register hook for this layer\n",
    "            attention_layer = model.model.layers[layer_idx].self_attn\n",
    "            hook = attention_layer.register_forward_hook(create_ablation_hook(head_num))\n",
    "            hooks.append((hook, layer_idx, head_num))\n",
    "        \n",
    "        print(f\"  Successfully added {len(hooks)} hooks. Running generation...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate rollouts with ALL heads ablated simultaneously\n",
    "            rollout_texts, rollout_answer_correct, rollout_answer_correct_check = generate_diverse_rollouts(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer, \n",
    "                ground_truth_answer=ground_truth_answer,\n",
    "                context=context_without,\n",
    "                num_rollouts=num_rollouts,\n",
    "                batch_size=batch_size,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                sentence_idx=sentence_idx\n",
    "            )\n",
    "            \n",
    "            # Process rollout embeddings (same as baseline)\n",
    "            rollout_sentences = [split_solution_into_chunks(rollout_text)[0] for rollout_text in rollout_texts]\n",
    "            \n",
    "            # Get all embeddings at once\n",
    "            rollout_llm_embeddings, rollout_sentence_embeddings = get_both_embeddings_batch(\n",
    "                model, sentence_model, tokenizer, rollout_sentences\n",
    "            )\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            cos_sims_llm = torch.cosine_similarity(\n",
    "                original_llm_emb.unsqueeze(0), rollout_llm_embeddings, dim=1\n",
    "            )\n",
    "            cos_sims_sent = torch.cosine_similarity(\n",
    "                original_sent_emb.unsqueeze(0), rollout_sentence_embeddings, dim=1\n",
    "            )\n",
    "            \n",
    "            # Calculate basic statistics\n",
    "            unique_responses = len(set(rollout_texts))\n",
    "            \n",
    "            # Store ablation result for ALL heads ablated together\n",
    "            ablation_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"function_tags\": function_tags,\n",
    "                \n",
    "                # Multi-head ablation info\n",
    "                \"ablated_heads\": receiver_heads,  # List of all ablated heads\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"num_ablated_heads\": len(receiver_heads),\n",
    "                \n",
    "                # Context information\n",
    "                \"context_without_sentence\": context_without,\n",
    "                \"ground_truth_answer\": ground_truth_answer,\n",
    "                \n",
    "                # Rollout data - SAME AS YOUR BASELINE\n",
    "                \"num_rollouts\": num_rollouts,\n",
    "                \"rollout_texts\": rollout_texts,  # Full rollout texts\n",
    "                \"rollout_sentences\": rollout_sentences,\n",
    "                \"rollout_answer_correct\": rollout_answer_correct,\n",
    "                \"rollout_answer_correct_check\": rollout_answer_correct_check,\n",
    "                \"unique_responses\": unique_responses,\n",
    "                \n",
    "                # Extracted boxed answers\n",
    "                \"rollout_boxed_answers\": [extract_boxed_answers(text) for text in rollout_texts],\n",
    "                \n",
    "                # Raw similarity scores\n",
    "                \"cos_sims_llm\": cos_sims_llm.cpu().tolist(),\n",
    "                \"cos_sims_sentence\": cos_sims_sent.cpu().tolist(),\n",
    "                \n",
    "                # Embedding comparison\n",
    "                \"embedding_correlation\": float(torch.corrcoef(torch.stack([cos_sims_llm, cos_sims_sent]))[0, 1]) if len(cos_sims_llm) > 1 else 0.0,\n",
    "                \n",
    "                # Metadata\n",
    "                \"generation_params\": {\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"batch_size\": batch_size\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            all_ablation_results.append(ablation_result)\n",
    "            \n",
    "            print(f\"  Successfully generated {len(rollout_texts)} rollouts with all heads ablated\")\n",
    "            print(f\"  Unique responses: {unique_responses}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del rollout_llm_embeddings, rollout_sentence_embeddings\n",
    "            del cos_sims_llm, cos_sims_sent, rollout_sentences\n",
    "            # rollout_texts will be garbage collected after saving\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error during multi-head ablation: {e}\")\n",
    "            \n",
    "            #  Emergency hook cleanup on error\n",
    "            print(f\"  Emergency hook cleanup - removing {len(hooks)} hooks...\")\n",
    "            for hook, layer_idx, head_num in hooks:\n",
    "                try:\n",
    "                    hook.remove()\n",
    "                except:\n",
    "                    pass  # Hook might already be removed\n",
    "            hooks.clear()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Store error result\n",
    "            error_result = {\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sentence_idx\": sentence_idx,\n",
    "                \"sentence_text\": original_text,\n",
    "                \"ablated_heads\": receiver_heads,\n",
    "                \"ablation_type\": \"multi_head_simultaneous\",\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "            all_ablation_results.append(error_result)\n",
    "            \n",
    "        finally:\n",
    "            # ALWAYS remove all hooks\n",
    "            print(f\"  Removing {len(hooks)} hooks...\")\n",
    "            for hook, layer_idx, head_num in hooks:\n",
    "                try:\n",
    "                    hook.remove()\n",
    "                except:\n",
    "                    pass  # Might already be removed in error handling\n",
    "            hooks.clear()\n",
    "        \n",
    "        #  SAVE AFTER EVERY SENTENCE (FIXED!)\n",
    "        print(f\"  Saving results after sentence {sentence_idx + 1}...\")\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_ablation_results, f, indent=2, default=str)  #  Save FULL list!\n",
    "        print(f\"  Results saved to: {results_file}\")\n",
    "        \n",
    "        # Cleanup after processing this sentence\n",
    "        del original_llm_emb, original_sent_emb\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Final save (this is now redundant but harmless)\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_ablation_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        \"problem_id\": problem_id,\n",
    "        \"total_sentences\": len(allsentences),\n",
    "        \"ablated_heads\": receiver_heads,\n",
    "        \"num_ablated_heads\": len(receiver_heads),\n",
    "        \"ablation_type\": \"multi_head_simultaneous\",\n",
    "        \"total_ablation_experiments\": len(allsentences),  # One experiment per sentence\n",
    "        \"successful_experiments\": len([r for r in all_ablation_results if \"error\" not in r]),\n",
    "        \"failed_experiments\": len([r for r in all_ablation_results if \"error\" in r]),\n",
    "        \"generation_params\": {\n",
    "            \"num_rollouts\": num_rollouts,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = problem_dir / \"multi_head_ablation_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nCompleted problem {problem_id}\")\n",
    "    print(f\"  - Total sentences: {len(allsentences)}\")\n",
    "    print(f\"  - Heads ablated simultaneously: {len(receiver_heads)}\")\n",
    "    print(f\"  - Total experiments: {len(allsentences)}\")\n",
    "    print(f\"  - Successful: {summary['successful_experiments']}\")\n",
    "    print(f\"  - Failed: {summary['failed_experiments']}\")\n",
    "    print(f\"  - Results saved to: {results_file}\")\n",
    "    \n",
    "    return all_ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b304ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing problem problem_6481 with 187 sentences\n",
      "Will ablate 20 receiver heads SIMULTANEOUSLY per sentence\n",
      "\n",
      "--- Processing sentence 187/187 ---\n",
      "Sentence: **Final Answer**\n",
      "The length of the boundary of the bolded figure is \\boxed{30.8}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chriskino/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Adding hooks for 20 heads...\n",
      "    Adding hook for head (29, 24)\n",
      "    Adding hook for head (17, 0)\n",
      "    Adding hook for head (24, 7)\n",
      "    Adding hook for head (25, 22)\n",
      "    Adding hook for head (23, 8)\n",
      "    Adding hook for head (18, 12)\n",
      "    Adding hook for head (23, 23)\n",
      "    Adding hook for head (21, 4)\n",
      "    Adding hook for head (19, 17)\n",
      "    Adding hook for head (18, 14)\n",
      "    Adding hook for head (30, 17)\n",
      "    Adding hook for head (19, 27)\n",
      "    Adding hook for head (28, 22)\n",
      "    Adding hook for head (1, 17)\n",
      "    Adding hook for head (27, 1)\n",
      "    Adding hook for head (24, 1)\n",
      "    Adding hook for head (26, 10)\n",
      "    Adding hook for head (26, 24)\n",
      "    Adding hook for head (1, 16)\n",
      "    Adding hook for head (24, 5)\n",
      "  Successfully added 20 hooks. Running generation...\n",
      "  Successfully generated 12 rollouts with all heads ablated\n",
      "  Unique responses: 12\n",
      "  Removing 20 hooks...\n",
      "  Saving results after sentence 187...\n",
      "  Results saved to: rollout_results_ablation_with_text/problem_6481/sentence_multi_head_ablation_rollouts.json\n",
      "\n",
      "Completed problem problem_6481\n",
      "  - Total sentences: 187\n",
      "  - Heads ablated simultaneously: 20\n",
      "  - Total experiments: 187\n",
      "  - Successful: 1\n",
      "  - Failed: 0\n",
      "  - Results saved to: rollout_results_ablation_with_text/problem_6481/sentence_multi_head_ablation_rollouts.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Run multi-head ablation for just the first problem\n",
    "results = process_single_problem_with_multi_head_ablation(\n",
    "    problem_data=all_prompt[0],\n",
    "    problem_labels=all_labels[0],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    sentence_model=sentence_model,\n",
    "    receiver_heads=reciever_heads,  # your list of (layer, head) tuples\n",
    "    output_dir=\"rollout_results_ablation_with_text\",\n",
    "    num_rollouts=12,\n",
    "    batch_size=6,\n",
    "    force=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
